{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "csv_file = 'fer2013.csv'\n",
    "labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = torchvision.models.resnet34(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FER 2013 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    \"\"\"FER dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, partition, transform=None):\n",
    "        self.csv = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.distribution = np.zeros(7, dtype=np.int16)\n",
    "        print(\"AVA Dataset initialization begin...\")\n",
    "        print(\"Rating distribution initialized: \", self.distribution)\n",
    "        \n",
    "        self.dataset = {}\n",
    "\n",
    "        self.idx = 0\n",
    "        for csv_idx, row in self.csv.iterrows(): \n",
    "            if row[2] == partition:\n",
    "                img = np.asarray([float(p) for p in row[1].split()]).reshape(48, 48)\n",
    "                self.dataset[self.idx] = [img, row[0]]\n",
    "                self.distribution[row[0]] += 1\n",
    "                self.idx += 1\n",
    "            if csv_idx % 5000 == 0:\n",
    "                print('csv_idx:', csv_idx, \" - Current distribution is: \", self.distribution)\n",
    "            \n",
    "        print(\"Initialization complete. Distribution is: \", self.distribution)\n",
    "\n",
    "        # Visualize categorical distribution\n",
    "        plt.bar(np.arange(7), self.distribution, 0.35) #(indeces, data, width)\n",
    "        plt.ylabel('Number of Pictures')\n",
    "        plt.title('Current Distribution')\n",
    "        plt.xticks(np.arange(7), ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'))\n",
    "        plt.show()\n",
    "        \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx][0]\n",
    "        category = self.dataset[idx][1]\n",
    "        sample = {'image': np.array(img/255, dtype=float), 'category': np.array(category, dtype=float)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['category']\n",
    "        rating = np.array(rating)\n",
    "        # image = image.transpose((2, 0, 1)) #swap color axis because: numpy image: H x W x C & torch image: C X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                 'category': torch.from_numpy(rating)}\n",
    "    \n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels]\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA Dataset initialization begin...\n",
      "Rating distribution initialized:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 0  - Current distribution is:  [1 0 0 0 0 0 0]\n",
      "csv_idx: 5000  - Current distribution is:  [ 714   72  726 1279  817  525  868]\n",
      "csv_idx: 10000  - Current distribution is:  [1407  160 1431 2538 1663 1091 1711]\n",
      "csv_idx: 15000  - Current distribution is:  [2066  239 2176 3742 2548 1598 2632]\n",
      "csv_idx: 20000  - Current distribution is:  [2803  324 2916 5009 3357 2132 3460]\n",
      "csv_idx: 25000  - Current distribution is:  [3455  389 3609 6297 4209 2721 4321]\n",
      "csv_idx: 30000  - Current distribution is:  [3995  436 4097 7215 4830 3171 4965]\n",
      "csv_idx: 35000  - Current distribution is:  [3995  436 4097 7215 4830 3171 4965]\n",
      "Initialization complete. Distribution is:  [3995  436 4097 7215 4830 3171 4965]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA Dataset initialization begin...\n",
      "Rating distribution initialized:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 0  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 5000  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 10000  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 15000  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 20000  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 25000  - Current distribution is:  [0 0 0 0 0 0 0]\n",
      "csv_idx: 30000  - Current distribution is:  [178  21 196 306 220 158 213]\n",
      "csv_idx: 35000  - Current distribution is:  [467  56 496 895 653 415 607]\n",
      "Initialization complete. Distribution is:  [467  56 496 895 653 415 607]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = FERDataset(csv_file=csv_file, partition='Training', transform=ToTensor())\n",
    "dataset_val = FERDataset(csv_file=csv_file, partition='PublicTest', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmoNet, self).__init__()\n",
    "        self.Res = resnet34\n",
    "        self.Res.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        self.Res.fc = nn.Linear(512, 512)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 7)\n",
    "        self.Softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Res(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.Softmax(x)\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"EmoNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=[3,3], stride=1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=[5,5], stride=1)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=[3,3], stride=1)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=[3,3], stride=1)\n",
    "        self.activation4 = nn.ReLU()\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation5 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(4608, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7)\n",
    "        \n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.activation4(x)\n",
    "        \n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.activation5(x)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.Softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmoNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.00001, betas=(0.9, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for p in model.Res.conv1.parameters():\n",
    "    p.requires_grad=True\n",
    "for p in model.Res.layer1.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.Res.layer2.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.Res.layer3.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.Res.layer4.parameters():\n",
    "    p.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = {}\n",
    "training_stats['tarining_loss'] = []\n",
    "training_stats['validation_loss'] = []\n",
    "training_stats['accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=[3, 3], stride=(1, 1))\n",
       "  (activation1): ReLU()\n",
       "  (conv2): Conv2d(32, 64, kernel_size=[5, 5], stride=(1, 1))\n",
       "  (activation2): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=[2, 2], stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1))\n",
       "  (activation3): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=[2, 2], stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1))\n",
       "  (activation4): ReLU()\n",
       "  (pool3): MaxPool2d(kernel_size=[2, 2], stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
       "  (activation5): ReLU()\n",
       "  (fc1): Linear(in_features=4608, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  (ReLU): ReLU()\n",
       "  (Softmax): Softmax(dim=0)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "writer = SummaryWriter('TensorBoard/EmoNet')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "sample = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(sample['image'])\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_images', img_grid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 0, batch index: 225, train loss: 1.945234\n",
      ">>> epoch: 0, batch index: 29, test loss: 1.844869, acc: 0.138\n",
      "==================================================================\n",
      ">>> epoch: 1, batch index: 225, train loss: 1.879214\n",
      ">>> epoch: 1, batch index: 29, test loss: 1.853772, acc: 0.148\n",
      "==================================================================\n",
      ">>> epoch: 2, batch index: 225, train loss: 1.906417\n",
      ">>> epoch: 2, batch index: 29, test loss: 1.846371, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 3, batch index: 225, train loss: 1.949440\n",
      ">>> epoch: 3, batch index: 29, test loss: 1.818681, acc: 0.144\n",
      "==================================================================\n",
      ">>> epoch: 4, batch index: 225, train loss: 1.944559\n",
      ">>> epoch: 4, batch index: 29, test loss: 1.814666, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 5, batch index: 225, train loss: 1.874826\n",
      ">>> epoch: 5, batch index: 29, test loss: 1.852182, acc: 0.144\n",
      "==================================================================\n",
      ">>> epoch: 6, batch index: 225, train loss: 1.890322\n",
      ">>> epoch: 6, batch index: 29, test loss: 1.860910, acc: 0.135\n",
      "==================================================================\n",
      ">>> epoch: 7, batch index: 225, train loss: 1.869836\n",
      ">>> epoch: 7, batch index: 29, test loss: 1.868903, acc: 0.143\n",
      "==================================================================\n",
      ">>> epoch: 8, batch index: 225, train loss: 1.901701\n",
      ">>> epoch: 8, batch index: 29, test loss: 1.856545, acc: 0.145\n",
      "==================================================================\n",
      ">>> epoch: 9, batch index: 225, train loss: 1.921874\n",
      ">>> epoch: 9, batch index: 29, test loss: 1.847996, acc: 0.141\n",
      "==================================================================\n",
      ">>> epoch: 10, batch index: 225, train loss: 1.920209\n",
      ">>> epoch: 10, batch index: 29, test loss: 1.845703, acc: 0.142\n",
      "==================================================================\n",
      ">>> epoch: 11, batch index: 225, train loss: 1.895072\n",
      ">>> epoch: 11, batch index: 29, test loss: 1.842049, acc: 0.151\n",
      "==================================================================\n",
      ">>> epoch: 12, batch index: 225, train loss: 1.919308\n",
      ">>> epoch: 12, batch index: 29, test loss: 1.819269, acc: 0.153\n",
      "==================================================================\n",
      ">>> epoch: 13, batch index: 225, train loss: 1.893968\n",
      ">>> epoch: 13, batch index: 29, test loss: 1.821073, acc: 0.147\n",
      "==================================================================\n",
      ">>> epoch: 14, batch index: 225, train loss: 1.836001\n",
      ">>> epoch: 14, batch index: 29, test loss: 1.819024, acc: 0.139\n",
      "==================================================================\n",
      ">>> epoch: 15, batch index: 225, train loss: 1.887002\n",
      ">>> epoch: 15, batch index: 29, test loss: 1.844269, acc: 0.145\n",
      "==================================================================\n",
      ">>> epoch: 16, batch index: 225, train loss: 1.918471\n",
      ">>> epoch: 16, batch index: 29, test loss: 1.827972, acc: 0.142\n",
      "==================================================================\n",
      ">>> epoch: 17, batch index: 225, train loss: 1.896121\n",
      ">>> epoch: 17, batch index: 29, test loss: 1.824371, acc: 0.140\n",
      "==================================================================\n",
      ">>> epoch: 18, batch index: 225, train loss: 1.880472\n",
      ">>> epoch: 18, batch index: 29, test loss: 1.856440, acc: 0.149\n",
      "==================================================================\n",
      ">>> epoch: 19, batch index: 225, train loss: 1.881932\n",
      ">>> epoch: 19, batch index: 29, test loss: 1.826331, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 20, batch index: 225, train loss: 1.882668\n",
      ">>> epoch: 20, batch index: 29, test loss: 1.826655, acc: 0.140\n",
      "==================================================================\n",
      ">>> epoch: 21, batch index: 225, train loss: 1.891352\n",
      ">>> epoch: 21, batch index: 29, test loss: 1.825357, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 22, batch index: 225, train loss: 1.910837\n",
      ">>> epoch: 22, batch index: 29, test loss: 1.846584, acc: 0.136\n",
      "==================================================================\n",
      ">>> epoch: 23, batch index: 225, train loss: 1.897884\n",
      ">>> epoch: 23, batch index: 29, test loss: 1.847843, acc: 0.144\n",
      "==================================================================\n",
      ">>> epoch: 24, batch index: 225, train loss: 1.914400\n",
      ">>> epoch: 24, batch index: 29, test loss: 1.851963, acc: 0.142\n",
      "==================================================================\n",
      ">>> epoch: 25, batch index: 225, train loss: 1.923378\n",
      ">>> epoch: 25, batch index: 29, test loss: 1.826579, acc: 0.142\n",
      "==================================================================\n",
      ">>> epoch: 26, batch index: 225, train loss: 1.951589\n",
      ">>> epoch: 26, batch index: 29, test loss: 1.861464, acc: 0.135\n",
      "==================================================================\n",
      ">>> epoch: 27, batch index: 225, train loss: 1.883709\n",
      ">>> epoch: 27, batch index: 29, test loss: 1.829756, acc: 0.134\n",
      "==================================================================\n",
      ">>> epoch: 28, batch index: 225, train loss: 1.857698\n",
      ">>> epoch: 28, batch index: 29, test loss: 1.830888, acc: 0.141\n",
      "==================================================================\n",
      ">>> epoch: 29, batch index: 225, train loss: 1.900404\n",
      ">>> epoch: 29, batch index: 29, test loss: 1.831564, acc: 0.136\n",
      "==================================================================\n",
      ">>> epoch: 30, batch index: 225, train loss: 1.857518\n",
      ">>> epoch: 30, batch index: 29, test loss: 1.864712, acc: 0.138\n",
      "==================================================================\n",
      ">>> epoch: 31, batch index: 225, train loss: 1.914458\n",
      ">>> epoch: 31, batch index: 29, test loss: 1.825143, acc: 0.148\n",
      "==================================================================\n",
      ">>> epoch: 32, batch index: 225, train loss: 1.874930\n",
      ">>> epoch: 32, batch index: 29, test loss: 1.812148, acc: 0.140\n",
      "==================================================================\n",
      ">>> epoch: 33, batch index: 225, train loss: 1.909690\n",
      ">>> epoch: 33, batch index: 29, test loss: 1.840411, acc: 0.137\n",
      "==================================================================\n",
      ">>> epoch: 34, batch index: 225, train loss: 1.933168\n",
      ">>> epoch: 34, batch index: 29, test loss: 1.848928, acc: 0.151\n",
      "==================================================================\n",
      ">>> epoch: 35, batch index: 225, train loss: 1.934586\n",
      ">>> epoch: 35, batch index: 29, test loss: 1.851292, acc: 0.140\n",
      "==================================================================\n",
      ">>> epoch: 36, batch index: 225, train loss: 1.885112\n",
      ">>> epoch: 36, batch index: 29, test loss: 1.831741, acc: 0.135\n",
      "==================================================================\n",
      ">>> epoch: 37, batch index: 225, train loss: 1.856847\n",
      ">>> epoch: 37, batch index: 29, test loss: 1.826055, acc: 0.138\n",
      "==================================================================\n",
      ">>> epoch: 38, batch index: 225, train loss: 1.905046\n",
      ">>> epoch: 38, batch index: 29, test loss: 1.829672, acc: 0.138\n",
      "==================================================================\n",
      ">>> epoch: 39, batch index: 225, train loss: 1.873388\n",
      ">>> epoch: 39, batch index: 29, test loss: 1.830535, acc: 0.141\n",
      "==================================================================\n",
      ">>> epoch: 40, batch index: 225, train loss: 1.939966\n",
      ">>> epoch: 40, batch index: 29, test loss: 1.828779, acc: 0.142\n",
      "==================================================================\n",
      ">>> epoch: 41, batch index: 225, train loss: 1.915293\n",
      ">>> epoch: 41, batch index: 29, test loss: 1.864354, acc: 0.149\n",
      "==================================================================\n",
      ">>> epoch: 42, batch index: 225, train loss: 1.887137\n",
      ">>> epoch: 42, batch index: 29, test loss: 1.828314, acc: 0.147\n",
      "==================================================================\n",
      ">>> epoch: 43, batch index: 225, train loss: 1.928513\n",
      ">>> epoch: 43, batch index: 29, test loss: 1.840267, acc: 0.145\n",
      "==================================================================\n",
      ">>> epoch: 44, batch index: 225, train loss: 1.890931\n",
      ">>> epoch: 44, batch index: 29, test loss: 1.830044, acc: 0.145\n",
      "==================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 45, batch index: 225, train loss: 1.913121\n",
      ">>> epoch: 45, batch index: 29, test loss: 1.859775, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 46, batch index: 225, train loss: 1.936648\n",
      ">>> epoch: 46, batch index: 29, test loss: 1.848428, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 47, batch index: 225, train loss: 1.917588\n",
      ">>> epoch: 47, batch index: 29, test loss: 1.817598, acc: 0.152\n",
      "==================================================================\n",
      ">>> epoch: 48, batch index: 225, train loss: 1.911946\n",
      ">>> epoch: 48, batch index: 29, test loss: 1.812623, acc: 0.157\n",
      "==================================================================\n",
      ">>> epoch: 49, batch index: 225, train loss: 1.883856\n",
      ">>> epoch: 49, batch index: 29, test loss: 1.837545, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 50, batch index: 225, train loss: 1.901214\n",
      ">>> epoch: 50, batch index: 29, test loss: 1.830904, acc: 0.156\n",
      "==================================================================\n",
      ">>> epoch: 51, batch index: 225, train loss: 1.905316\n",
      ">>> epoch: 51, batch index: 29, test loss: 1.844968, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 52, batch index: 225, train loss: 1.903171\n",
      ">>> epoch: 52, batch index: 29, test loss: 1.835458, acc: 0.155\n",
      "==================================================================\n",
      ">>> epoch: 53, batch index: 225, train loss: 1.950059\n",
      ">>> epoch: 53, batch index: 29, test loss: 1.862488, acc: 0.143\n",
      "==================================================================\n",
      ">>> epoch: 54, batch index: 225, train loss: 1.919763\n",
      ">>> epoch: 54, batch index: 29, test loss: 1.831565, acc: 0.152\n",
      "==================================================================\n",
      ">>> epoch: 55, batch index: 225, train loss: 1.880881\n",
      ">>> epoch: 55, batch index: 29, test loss: 1.843427, acc: 0.149\n",
      "==================================================================\n",
      ">>> epoch: 56, batch index: 225, train loss: 1.902631\n",
      ">>> epoch: 56, batch index: 29, test loss: 1.857904, acc: 0.147\n",
      "==================================================================\n",
      ">>> epoch: 57, batch index: 225, train loss: 1.910568\n",
      ">>> epoch: 57, batch index: 29, test loss: 1.848088, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 58, batch index: 225, train loss: 1.934377\n",
      ">>> epoch: 58, batch index: 29, test loss: 1.843040, acc: 0.149\n",
      "==================================================================\n",
      ">>> epoch: 59, batch index: 225, train loss: 1.948936\n",
      ">>> epoch: 59, batch index: 29, test loss: 1.825583, acc: 0.151\n",
      "==================================================================\n",
      ">>> epoch: 60, batch index: 225, train loss: 1.904641\n",
      ">>> epoch: 60, batch index: 29, test loss: 1.839612, acc: 0.149\n",
      "==================================================================\n",
      ">>> epoch: 61, batch index: 225, train loss: 1.898753\n",
      ">>> epoch: 61, batch index: 29, test loss: 1.829922, acc: 0.156\n",
      "==================================================================\n",
      ">>> epoch: 62, batch index: 225, train loss: 1.945055\n",
      ">>> epoch: 62, batch index: 29, test loss: 1.850632, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 63, batch index: 225, train loss: 1.908713\n",
      ">>> epoch: 63, batch index: 29, test loss: 1.833674, acc: 0.155\n",
      "==================================================================\n",
      ">>> epoch: 64, batch index: 225, train loss: 1.910267\n",
      ">>> epoch: 64, batch index: 29, test loss: 1.843790, acc: 0.158\n",
      "==================================================================\n",
      ">>> epoch: 65, batch index: 225, train loss: 1.890262\n",
      ">>> epoch: 65, batch index: 29, test loss: 1.851274, acc: 0.146\n",
      "==================================================================\n",
      ">>> epoch: 66, batch index: 225, train loss: 1.929284\n",
      ">>> epoch: 66, batch index: 29, test loss: 1.812728, acc: 0.155\n",
      "==================================================================\n",
      ">>> epoch: 67, batch index: 225, train loss: 1.896084\n",
      ">>> epoch: 67, batch index: 29, test loss: 1.852233, acc: 0.155\n",
      "==================================================================\n",
      ">>> epoch: 68, batch index: 225, train loss: 1.898021\n",
      ">>> epoch: 68, batch index: 29, test loss: 1.851795, acc: 0.159\n",
      "==================================================================\n",
      ">>> epoch: 69, batch index: 225, train loss: 1.880194\n",
      ">>> epoch: 69, batch index: 29, test loss: 1.843841, acc: 0.154\n",
      "==================================================================\n",
      ">>> epoch: 70, batch index: 225, train loss: 1.938453\n",
      ">>> epoch: 70, batch index: 29, test loss: 1.826631, acc: 0.157\n",
      "==================================================================\n",
      ">>> epoch: 71, batch index: 225, train loss: 1.875147\n",
      ">>> epoch: 71, batch index: 29, test loss: 1.843888, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 72, batch index: 225, train loss: 1.890410\n",
      ">>> epoch: 72, batch index: 29, test loss: 1.826549, acc: 0.150\n",
      "==================================================================\n",
      ">>> epoch: 73, batch index: 225, train loss: 1.902935\n",
      ">>> epoch: 73, batch index: 29, test loss: 1.825695, acc: 0.158\n",
      "==================================================================\n",
      ">>> epoch: 74, batch index: 225, train loss: 1.922321\n",
      ">>> epoch: 74, batch index: 29, test loss: 1.866831, acc: 0.156\n",
      "==================================================================\n",
      ">>> epoch: 75, batch index: 225, train loss: 1.889212\n",
      ">>> epoch: 75, batch index: 29, test loss: 1.828146, acc: 0.161\n",
      "==================================================================\n",
      ">>> epoch: 76, batch index: 225, train loss: 1.947190\n",
      ">>> epoch: 76, batch index: 29, test loss: 1.814896, acc: 0.159\n",
      "==================================================================\n",
      ">>> epoch: 77, batch index: 225, train loss: 1.877782\n",
      ">>> epoch: 77, batch index: 29, test loss: 1.846811, acc: 0.163\n",
      "==================================================================\n",
      ">>> epoch: 78, batch index: 225, train loss: 1.895592\n",
      ">>> epoch: 78, batch index: 29, test loss: 1.845803, acc: 0.156\n",
      "==================================================================\n",
      ">>> epoch: 79, batch index: 225, train loss: 1.901190\n",
      ">>> epoch: 79, batch index: 29, test loss: 1.818732, acc: 0.165\n",
      "==================================================================\n",
      ">>> epoch: 80, batch index: 225, train loss: 1.917472\n",
      ">>> epoch: 80, batch index: 29, test loss: 1.832006, acc: 0.143\n",
      "==================================================================\n",
      ">>> epoch: 81, batch index: 225, train loss: 1.906214\n",
      ">>> epoch: 81, batch index: 29, test loss: 1.847735, acc: 0.155\n",
      "==================================================================\n",
      ">>> epoch: 82, batch index: 225, train loss: 1.919843\n",
      ">>> epoch: 82, batch index: 29, test loss: 1.841980, acc: 0.172\n",
      "==================================================================\n",
      ">>> epoch: 83, batch index: 225, train loss: 1.904270\n",
      ">>> epoch: 83, batch index: 29, test loss: 1.829374, acc: 0.164\n",
      "==================================================================\n",
      ">>> epoch: 84, batch index: 225, train loss: 1.922886\n",
      ">>> epoch: 84, batch index: 29, test loss: 1.794423, acc: 0.159\n",
      "==================================================================\n",
      ">>> epoch: 85, batch index: 225, train loss: 1.909503\n",
      ">>> epoch: 85, batch index: 29, test loss: 1.848853, acc: 0.163\n",
      "==================================================================\n",
      ">>> epoch: 86, batch index: 225, train loss: 1.922048\n",
      ">>> epoch: 86, batch index: 29, test loss: 1.836879, acc: 0.154\n",
      "==================================================================\n",
      ">>> epoch: 87, batch index: 225, train loss: 1.825490\n",
      ">>> epoch: 87, batch index: 29, test loss: 1.826355, acc: 0.164\n",
      "==================================================================\n",
      ">>> epoch: 88, batch index: 225, train loss: 1.875113\n",
      ">>> epoch: 88, batch index: 29, test loss: 1.833416, acc: 0.159\n",
      "==================================================================\n",
      ">>> epoch: 89, batch index: 225, train loss: 1.880498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 89, batch index: 29, test loss: 1.827748, acc: 0.160\n",
      "==================================================================\n",
      ">>> epoch: 90, batch index: 225, train loss: 1.928461\n",
      ">>> epoch: 90, batch index: 29, test loss: 1.811294, acc: 0.168\n",
      "==================================================================\n",
      ">>> epoch: 91, batch index: 225, train loss: 1.903725\n",
      ">>> epoch: 91, batch index: 29, test loss: 1.845224, acc: 0.165\n",
      "==================================================================\n",
      ">>> epoch: 92, batch index: 225, train loss: 1.894101\n",
      ">>> epoch: 92, batch index: 29, test loss: 1.849460, acc: 0.159\n",
      "==================================================================\n",
      ">>> epoch: 93, batch index: 225, train loss: 1.908192\n",
      ">>> epoch: 93, batch index: 29, test loss: 1.809029, acc: 0.154\n",
      "==================================================================\n",
      ">>> epoch: 94, batch index: 225, train loss: 1.884688\n",
      ">>> epoch: 94, batch index: 29, test loss: 1.847642, acc: 0.160\n",
      "==================================================================\n",
      ">>> epoch: 95, batch index: 225, train loss: 1.878099\n",
      ">>> epoch: 95, batch index: 29, test loss: 1.810894, acc: 0.164\n",
      "==================================================================\n",
      ">>> epoch: 96, batch index: 225, train loss: 1.854112\n",
      ">>> epoch: 96, batch index: 29, test loss: 1.864940, acc: 0.166\n",
      "==================================================================\n",
      ">>> epoch: 97, batch index: 225, train loss: 1.920332\n",
      ">>> epoch: 97, batch index: 29, test loss: 1.827551, acc: 0.154\n",
      "==================================================================\n",
      ">>> epoch: 98, batch index: 225, train loss: 1.902457\n",
      ">>> epoch: 98, batch index: 29, test loss: 1.827260, acc: 0.163\n",
      "==================================================================\n",
      ">>> epoch: 99, batch index: 225, train loss: 1.883939\n",
      ">>> epoch: 99, batch index: 29, test loss: 1.828453, acc: 0.167\n",
      "==================================================================\n",
      ">>> epoch: 100, batch index: 225, train loss: 1.897261\n",
      ">>> epoch: 100, batch index: 29, test loss: 1.828994, acc: 0.166\n",
      "==================================================================\n",
      ">>> epoch: 101, batch index: 225, train loss: 1.918983\n",
      ">>> epoch: 101, batch index: 29, test loss: 1.836666, acc: 0.165\n",
      "==================================================================\n",
      ">>> epoch: 102, batch index: 225, train loss: 1.839623\n",
      ">>> epoch: 102, batch index: 29, test loss: 1.826643, acc: 0.160\n",
      "==================================================================\n",
      ">>> epoch: 103, batch index: 225, train loss: 1.916544\n",
      ">>> epoch: 103, batch index: 29, test loss: 1.848629, acc: 0.170\n",
      "==================================================================\n",
      ">>> epoch: 104, batch index: 225, train loss: 1.894587\n",
      ">>> epoch: 104, batch index: 29, test loss: 1.852122, acc: 0.162\n",
      "==================================================================\n",
      ">>> epoch: 105, batch index: 225, train loss: 1.895154\n",
      ">>> epoch: 105, batch index: 29, test loss: 1.848179, acc: 0.160\n",
      "==================================================================\n",
      ">>> epoch: 106, batch index: 225, train loss: 1.922020\n",
      ">>> epoch: 106, batch index: 29, test loss: 1.858430, acc: 0.169\n",
      "==================================================================\n",
      ">>> epoch: 107, batch index: 225, train loss: 1.932695\n",
      ">>> epoch: 107, batch index: 29, test loss: 1.825301, acc: 0.166\n",
      "==================================================================\n",
      ">>> epoch: 108, batch index: 225, train loss: 1.888668\n",
      ">>> epoch: 108, batch index: 29, test loss: 1.850927, acc: 0.166\n",
      "==================================================================\n",
      ">>> epoch: 109, batch index: 225, train loss: 1.853546\n",
      ">>> epoch: 109, batch index: 29, test loss: 1.847162, acc: 0.170\n",
      "==================================================================\n",
      ">>> epoch: 110, batch index: 225, train loss: 1.895964\n",
      ">>> epoch: 110, batch index: 29, test loss: 1.838285, acc: 0.164\n",
      "==================================================================\n",
      ">>> epoch: 111, batch index: 225, train loss: 1.870655\n",
      ">>> epoch: 111, batch index: 29, test loss: 1.821829, acc: 0.174\n",
      "==================================================================\n",
      ">>> epoch: 112, batch index: 225, train loss: 1.856112\n",
      ">>> epoch: 112, batch index: 29, test loss: 1.827947, acc: 0.169\n",
      "==================================================================\n",
      ">>> epoch: 113, batch index: 225, train loss: 1.901165\n",
      ">>> epoch: 113, batch index: 29, test loss: 1.868869, acc: 0.175\n",
      "==================================================================\n",
      ">>> epoch: 114, batch index: 225, train loss: 1.915337\n",
      ">>> epoch: 114, batch index: 29, test loss: 1.820842, acc: 0.164\n",
      "==================================================================\n",
      ">>> epoch: 115, batch index: 225, train loss: 1.905236\n",
      ">>> epoch: 115, batch index: 29, test loss: 1.847252, acc: 0.170\n",
      "==================================================================\n",
      ">>> epoch: 116, batch index: 225, train loss: 1.859640\n",
      ">>> epoch: 116, batch index: 29, test loss: 1.829325, acc: 0.163\n",
      "==================================================================\n",
      ">>> epoch: 117, batch index: 225, train loss: 1.883038\n",
      ">>> epoch: 117, batch index: 29, test loss: 1.828815, acc: 0.169\n",
      "==================================================================\n",
      ">>> epoch: 118, batch index: 225, train loss: 1.922839\n",
      ">>> epoch: 118, batch index: 29, test loss: 1.833561, acc: 0.179\n",
      "==================================================================\n",
      ">>> epoch: 119, batch index: 225, train loss: 1.895675\n",
      ">>> epoch: 119, batch index: 29, test loss: 1.867774, acc: 0.169\n",
      "==================================================================\n",
      ">>> epoch: 120, batch index: 225, train loss: 1.849763\n",
      ">>> epoch: 120, batch index: 29, test loss: 1.823852, acc: 0.171\n",
      "==================================================================\n",
      ">>> epoch: 121, batch index: 225, train loss: 1.864866\n",
      ">>> epoch: 121, batch index: 29, test loss: 1.827966, acc: 0.168\n",
      "==================================================================\n",
      ">>> epoch: 122, batch index: 225, train loss: 1.881777\n",
      ">>> epoch: 122, batch index: 29, test loss: 1.845951, acc: 0.172\n",
      "==================================================================\n",
      ">>> epoch: 123, batch index: 225, train loss: 1.899375\n",
      ">>> epoch: 123, batch index: 29, test loss: 1.864265, acc: 0.171\n",
      "==================================================================\n",
      ">>> epoch: 124, batch index: 225, train loss: 1.919950\n",
      ">>> epoch: 124, batch index: 29, test loss: 1.848417, acc: 0.176\n",
      "==================================================================\n",
      ">>> epoch: 125, batch index: 225, train loss: 1.906604\n",
      ">>> epoch: 125, batch index: 29, test loss: 1.844612, acc: 0.179\n",
      "==================================================================\n",
      ">>> epoch: 126, batch index: 225, train loss: 1.893560\n",
      ">>> epoch: 126, batch index: 29, test loss: 1.850229, acc: 0.174\n",
      "==================================================================\n",
      ">>> epoch: 127, batch index: 225, train loss: 1.917363\n",
      ">>> epoch: 127, batch index: 29, test loss: 1.855386, acc: 0.184\n",
      "==================================================================\n",
      ">>> epoch: 128, batch index: 225, train loss: 1.876934\n",
      ">>> epoch: 128, batch index: 29, test loss: 1.846592, acc: 0.177\n",
      "==================================================================\n",
      ">>> epoch: 129, batch index: 225, train loss: 1.902475\n",
      ">>> epoch: 129, batch index: 29, test loss: 1.844529, acc: 0.172\n",
      "==================================================================\n",
      ">>> epoch: 130, batch index: 225, train loss: 1.863799\n",
      ">>> epoch: 130, batch index: 29, test loss: 1.827676, acc: 0.182\n",
      "==================================================================\n",
      ">>> epoch: 131, batch index: 225, train loss: 1.833082\n",
      ">>> epoch: 131, batch index: 29, test loss: 1.811131, acc: 0.177\n",
      "==================================================================\n",
      ">>> epoch: 132, batch index: 225, train loss: 1.901864\n",
      ">>> epoch: 132, batch index: 29, test loss: 1.845494, acc: 0.186\n",
      "==================================================================\n",
      ">>> epoch: 133, batch index: 225, train loss: 1.884755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 133, batch index: 29, test loss: 1.825975, acc: 0.175\n",
      "==================================================================\n",
      ">>> epoch: 134, batch index: 225, train loss: 1.845619\n",
      ">>> epoch: 134, batch index: 29, test loss: 1.830744, acc: 0.178\n",
      "==================================================================\n",
      ">>> epoch: 135, batch index: 225, train loss: 1.890840\n",
      ">>> epoch: 135, batch index: 29, test loss: 1.813682, acc: 0.182\n",
      "==================================================================\n",
      ">>> epoch: 136, batch index: 225, train loss: 1.939739\n",
      ">>> epoch: 136, batch index: 29, test loss: 1.843533, acc: 0.181\n",
      "==================================================================\n",
      ">>> epoch: 137, batch index: 225, train loss: 1.884863\n",
      ">>> epoch: 137, batch index: 29, test loss: 1.829578, acc: 0.182\n",
      "==================================================================\n",
      ">>> epoch: 138, batch index: 225, train loss: 1.941773\n",
      ">>> epoch: 138, batch index: 29, test loss: 1.827722, acc: 0.177\n",
      "==================================================================\n",
      ">>> epoch: 139, batch index: 225, train loss: 1.889656\n",
      ">>> epoch: 139, batch index: 29, test loss: 1.837995, acc: 0.178\n",
      "==================================================================\n",
      ">>> epoch: 140, batch index: 225, train loss: 1.902238\n",
      ">>> epoch: 140, batch index: 29, test loss: 1.826638, acc: 0.184\n",
      "==================================================================\n",
      ">>> epoch: 141, batch index: 225, train loss: 1.918020\n",
      ">>> epoch: 141, batch index: 29, test loss: 1.850009, acc: 0.179\n",
      "==================================================================\n",
      ">>> epoch: 142, batch index: 225, train loss: 1.908149\n",
      ">>> epoch: 142, batch index: 29, test loss: 1.825870, acc: 0.180\n",
      "==================================================================\n",
      ">>> epoch: 143, batch index: 225, train loss: 1.888664\n",
      ">>> epoch: 143, batch index: 29, test loss: 1.863857, acc: 0.181\n",
      "==================================================================\n",
      ">>> epoch: 144, batch index: 225, train loss: 1.875206\n",
      ">>> epoch: 144, batch index: 29, test loss: 1.827128, acc: 0.174\n",
      "==================================================================\n",
      ">>> epoch: 145, batch index: 225, train loss: 1.862007\n",
      ">>> epoch: 145, batch index: 29, test loss: 1.849174, acc: 0.177\n",
      "==================================================================\n",
      ">>> epoch: 146, batch index: 225, train loss: 1.883430\n",
      ">>> epoch: 146, batch index: 29, test loss: 1.846930, acc: 0.184\n",
      "==================================================================\n",
      ">>> epoch: 147, batch index: 225, train loss: 1.866224\n",
      ">>> epoch: 147, batch index: 29, test loss: 1.833291, acc: 0.181\n",
      "==================================================================\n",
      ">>> epoch: 148, batch index: 225, train loss: 1.895936\n",
      ">>> epoch: 148, batch index: 29, test loss: 1.844606, acc: 0.181\n",
      "==================================================================\n",
      ">>> epoch: 149, batch index: 225, train loss: 1.930651\n",
      ">>> epoch: 149, batch index: 29, test loss: 1.827753, acc: 0.183\n",
      "==================================================================\n",
      ">>> epoch: 150, batch index: 225, train loss: 1.877055\n",
      ">>> epoch: 150, batch index: 29, test loss: 1.867664, acc: 0.191\n",
      "==================================================================\n",
      ">>> epoch: 151, batch index: 225, train loss: 1.910426\n",
      ">>> epoch: 151, batch index: 29, test loss: 1.842650, acc: 0.185\n",
      "==================================================================\n",
      ">>> epoch: 152, batch index: 225, train loss: 1.899867\n",
      ">>> epoch: 152, batch index: 29, test loss: 1.827447, acc: 0.185\n",
      "==================================================================\n",
      ">>> epoch: 153, batch index: 225, train loss: 1.898655\n",
      ">>> epoch: 153, batch index: 29, test loss: 1.850114, acc: 0.192\n",
      "==================================================================\n",
      ">>> epoch: 154, batch index: 225, train loss: 1.869954\n",
      ">>> epoch: 154, batch index: 29, test loss: 1.825325, acc: 0.187\n",
      "==================================================================\n",
      ">>> epoch: 155, batch index: 225, train loss: 1.891156\n",
      ">>> epoch: 155, batch index: 29, test loss: 1.822563, acc: 0.190\n",
      "==================================================================\n",
      ">>> epoch: 156, batch index: 225, train loss: 1.909133\n",
      ">>> epoch: 156, batch index: 29, test loss: 1.830341, acc: 0.188\n",
      "==================================================================\n",
      ">>> epoch: 157, batch index: 225, train loss: 1.890946\n",
      ">>> epoch: 157, batch index: 29, test loss: 1.844869, acc: 0.198\n",
      "==================================================================\n",
      ">>> epoch: 158, batch index: 225, train loss: 1.900379\n",
      ">>> epoch: 158, batch index: 29, test loss: 1.851868, acc: 0.194\n",
      "==================================================================\n",
      ">>> epoch: 159, batch index: 225, train loss: 1.880927\n",
      ">>> epoch: 159, batch index: 29, test loss: 1.865180, acc: 0.192\n",
      "==================================================================\n",
      ">>> epoch: 160, batch index: 225, train loss: 1.893867\n",
      ">>> epoch: 160, batch index: 29, test loss: 1.808454, acc: 0.196\n",
      "==================================================================\n",
      ">>> epoch: 161, batch index: 225, train loss: 1.879956\n",
      ">>> epoch: 161, batch index: 29, test loss: 1.825765, acc: 0.189\n",
      "==================================================================\n",
      ">>> epoch: 162, batch index: 225, train loss: 1.918572\n",
      ">>> epoch: 162, batch index: 29, test loss: 1.836915, acc: 0.193\n",
      "==================================================================\n",
      ">>> epoch: 163, batch index: 225, train loss: 1.901143\n",
      ">>> epoch: 163, batch index: 29, test loss: 1.804471, acc: 0.191\n",
      "==================================================================\n",
      ">>> epoch: 164, batch index: 225, train loss: 1.856925\n",
      ">>> epoch: 164, batch index: 29, test loss: 1.869149, acc: 0.195\n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "global epoch #declear epoch global, to be used later by torch.save() \n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    ave_loss = 0\n",
    "    global loss #declear loss global, to be used later by torch.save() \n",
    "    for batch_idx, diction in enumerate(train_loader):\n",
    "        model.train() #set model to traning mode\n",
    "        optimizer.zero_grad()\n",
    "        x, target = diction['image'], diction['category'] #extract training data for this batch\n",
    "        x = x.unsqueeze(1)\n",
    "        x, target = x.float(), target.float() #set datatype\n",
    "        x, target = x.to(device), target.to(device) #transfer to GPU\n",
    "        x, target = Variable(x), Variable(target) #set to pytorch datatype: variable\n",
    "        \n",
    "        out = model(x) #forward pass\n",
    "\n",
    "        loss = criterion(out, target.long()) #calculate loss\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1 \n",
    "        loss.backward() #back propagation with calculated loss\n",
    "        optimizer.step() #calculate gradient and step\n",
    "        \n",
    "        if (batch_idx + 1) % 600 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print('>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx + 1, loss))\n",
    "        training_stats['tarining_loss'].append(loss)\n",
    "        \n",
    "        \n",
    "    correct, ave_loss, total_cnt = 0, 0, 0\n",
    "    for batch_idx, diction in enumerate(val_loader):\n",
    "        model.eval() #set model to evaluation mode\n",
    "        x, target = diction['image'], diction['category']\n",
    "        x = x.unsqueeze(1)\n",
    "        x, target = x.float(), target.float()\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target.long()) #calculate loss\n",
    "\n",
    "        pred_label = out.data\n",
    "        pred_label = pred_label.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        target = target.long()\n",
    "        \n",
    "        total_cnt += x.data.size()[0]\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1 #smooth average\n",
    "        correct += pred_label.eq(target.view_as(pred_label)).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 600 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "            print(\n",
    "            '>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct * 1.0 / total_cnt))\n",
    "            print('==================================================================') \n",
    "            \n",
    "        training_stats['validation_loss'].append(ave_loss)\n",
    "        training_stats['accuracy'].append(correct * 1.0 / total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pthflops import count_ops\n",
    "\n",
    "inp = torch.Tensor(np.zeros(57600).reshape(25, 1, 48, 48))\n",
    "\n",
    "inp = inp.cuda()\n",
    "inp = inp.to(device)\n",
    "\n",
    "count_ops(model, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['tarining_loss'])), training_stats['tarining_loss'])\n",
    "plt.ylabel('training_loss')\n",
    "plt.title('iter')\n",
    "'''\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['validation_loss'])), training_stats['validation_loss'], 'ro')\n",
    "plt.ylabel('validation_loss')\n",
    "plt.title('iter')\n",
    "\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['accuracy'])), training_stats['accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('iter')\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(training_stats['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
