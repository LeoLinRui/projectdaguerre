{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "csv_file = 'fer2013.csv'\n",
    "labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = torchvision.models.resnet34(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FER 2013 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    \"\"\"FER dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, partition, transform=None, class_size=None):\n",
    "        self.csv = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.class_size = class_size\n",
    "\n",
    "        self.distribution = np.zeros(7, dtype=np.int16)\n",
    "        print(\"AVA Dataset initialization begin...\")\n",
    "        print(\"Rating distribution initialized: \", self.distribution)\n",
    "        \n",
    "        self.dataset = {}\n",
    "\n",
    "        self.idx = 0\n",
    "        for csv_idx, row in self.csv.iterrows(): \n",
    "            if row[2] == partition:\n",
    "                img = np.asarray([float(p) for p in row[1].split()]).reshape(48, 48)\n",
    "                self.dataset[self.idx] = [img, row[0]]\n",
    "                self.distribution[row[0]] += 1\n",
    "                self.idx += 1\n",
    "            # if csv_idx % 10000 == 0:\n",
    "                # print('csv_idx:', csv_idx, \" - Current distribution is: \", self.distribution)\n",
    "            \n",
    "        print(\"Initialization complete. Distribution is: \", self.distribution)\n",
    "\n",
    "        \n",
    "        if class_size: #only proceed when it's needed to limit class amount\n",
    "            self.distribution_multiplier = np.ceil(np.divide(np.full(7, self.class_size), self.distribution))\n",
    "            #np.ceil rounds up to provide enough images in each class. np.fill creates an array with 7 elements = class_size\n",
    "            print(\"Stage 2 begin. Target class size is:\", class_size, \"Distribution multiplier is:\", self.distribution_multiplier)\n",
    "            self.additional_diction = {}\n",
    "            for item in self.dataset.items(): #traverse the self.dataset\n",
    "                \n",
    "                img = item[1][0]\n",
    "                category = item[1][1]\n",
    "\n",
    "                if self.distribution[category] < class_size: #only enter if < class_limit\n",
    "                    for i in range(int(self.distribution_multiplier[category])):\n",
    "                        self.additional_diction[len(self.dataset) + len(self.additional_diction)] = [img, category]\n",
    "                        #append the same image at the end of additional_diction (distribution_multiplier) times\n",
    "                    self.distribution[category] += self.distribution_multiplier[category] #update rat_distribution\n",
    "            self.dataset.update(self.additional_diction) #combine diction and additional_diction\n",
    "        \n",
    "        \n",
    "        # Visualize categorical distribution\n",
    "        plt.bar(np.arange(7), self.distribution, 0.35) #(indeces, data, width)\n",
    "        plt.ylabel('Number of Pictures')\n",
    "        plt.title('Current Distribution')\n",
    "        plt.xticks(np.arange(7), ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral'))\n",
    "        plt.show()\n",
    "        \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.dataset[idx][0]\n",
    "        category = self.dataset[idx][1]\n",
    "        sample = {'image': torch.from_numpy(np.array(img/255, dtype=float)), 'category': np.array(category, dtype=float)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['category']\n",
    "        rating = np.array(rating)\n",
    "        # image = image.transpose((2, 0, 1)) #swap color axis because: numpy image: H x W x C & torch image: C X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                 'category': torch.from_numpy(rating)}\n",
    "    \n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "    y = torch.eye(num_classes) \n",
    "    return y[labels]\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA Dataset initialization begin...\n",
      "Rating distribution initialized:  [0 0 0 0 0 0 0]\n",
      "Initialization complete. Distribution is:  [3995  436 4097 7215 4830 3171 4965]\n",
      "Stage 2 begin. Target class size is: 7000 Distribution multiplier is: [ 2. 17.  2.  1.  2.  3.  2.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVA Dataset initialization begin...\n",
      "Rating distribution initialized:  [0 0 0 0 0 0 0]\n",
      "Initialization complete. Distribution is:  [467  56 496 895 653 415 607]\n",
      "Stage 2 begin. Target class size is: 800 Distribution multiplier is: [ 2. 15.  2.  1.  2.  2.  2.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = FERDataset(csv_file=csv_file, partition='Training', class_size=7000)\n",
    "dataset_val = FERDataset(csv_file=csv_file, partition='PublicTest', class_size=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6706, 0.6706, 0.6706,  ..., 0.6706, 0.6706, 0.6706],\n",
      "        [0.6706, 0.6706, 0.6706,  ..., 0.6706, 0.6706, 0.6706],\n",
      "        [0.6706, 0.6706, 0.6706,  ..., 0.6706, 0.6706, 0.6706],\n",
      "        ...,\n",
      "        [0.6588, 0.6627, 0.6549,  ..., 0.5725, 0.5451, 0.5294],\n",
      "        [0.6588, 0.6627, 0.6510,  ..., 0.6118, 0.5961, 0.6157],\n",
      "        [0.6588, 0.6549, 0.6510,  ..., 0.6039, 0.5922, 0.5765]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1976cc62488>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, sample = next(enumerate(train_loader))\n",
    "print(sample['image'][0])\n",
    "plt.imshow(sample['image'][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmoNet, self).__init__()\n",
    "        self.Res = resnet34\n",
    "        self.Res.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.Res.fc = nn.Linear(512, 512)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 7)\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Res(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.Softmax(x)\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"EmoNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=[1,1], stride=1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=[5,5], stride=1)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = [3, 3], stride=2, padding=0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation3 = nn.ReLU()\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = [3, 3], stride=2, padding=0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=128, kernel_size=[5,5], stride=1, padding=2)\n",
    "        self.activation4 = nn.ReLU()\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = [3, 3], stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 7)\n",
    "        \n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.activation3(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.activation4(x)\n",
    "        \n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.Softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmoNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-6, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for p in model.Res.conv1.parameters():\n",
    "    p.requires_grad=True\n",
    "for p in model.Res.layer1.parameters():\n",
    "    p.requires_grad=True\n",
    "for p in model.Res.layer2.parameters():\n",
    "    p.requires_grad=True\n",
    "for p in model.Res.layer3.parameters():\n",
    "    p.requires_grad=True\n",
    "for p in model.Res.layer4.parameters():\n",
    "    p.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = {}\n",
    "training_stats['tarining_loss'] = []\n",
    "training_stats['validation_loss'] = []\n",
    "training_stats['accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmoNet(\n",
       "  (Res): ResNet(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ReLU): ReLU()\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
       "  (Softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "writer = SummaryWriter('TensorBoard/EmoNet')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "sample = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(sample['image'])\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_images', img_grid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 0, batch index: 385, train loss: 1.287593\n",
      ">>> epoch: 0, batch index: 45, test loss: 1.614493, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 1, batch index: 385, train loss: 1.346610\n",
      ">>> epoch: 1, batch index: 45, test loss: 1.619319, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 2, batch index: 385, train loss: 1.332668\n",
      ">>> epoch: 2, batch index: 45, test loss: 1.600401, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 3, batch index: 385, train loss: 1.312084\n",
      ">>> epoch: 3, batch index: 45, test loss: 1.624892, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 4, batch index: 385, train loss: 1.297834\n",
      ">>> epoch: 4, batch index: 45, test loss: 1.620407, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 5, batch index: 385, train loss: 1.294113\n",
      ">>> epoch: 5, batch index: 45, test loss: 1.621217, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 6, batch index: 385, train loss: 1.284656\n",
      ">>> epoch: 6, batch index: 45, test loss: 1.616807, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 7, batch index: 385, train loss: 1.297356\n",
      ">>> epoch: 7, batch index: 45, test loss: 1.597363, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 8, batch index: 385, train loss: 1.263856\n",
      ">>> epoch: 8, batch index: 45, test loss: 1.619827, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 9, batch index: 385, train loss: 1.296612\n",
      ">>> epoch: 9, batch index: 45, test loss: 1.614441, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 10, batch index: 385, train loss: 1.273999\n",
      ">>> epoch: 10, batch index: 45, test loss: 1.614369, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 11, batch index: 385, train loss: 1.226290\n",
      ">>> epoch: 11, batch index: 45, test loss: 1.600874, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 12, batch index: 385, train loss: 1.295453\n",
      ">>> epoch: 12, batch index: 45, test loss: 1.590336, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 13, batch index: 385, train loss: 1.237702\n",
      ">>> epoch: 13, batch index: 45, test loss: 1.608088, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 14, batch index: 385, train loss: 1.320288\n",
      ">>> epoch: 14, batch index: 45, test loss: 1.617511, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 15, batch index: 385, train loss: 1.302533\n",
      ">>> epoch: 15, batch index: 45, test loss: 1.608119, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 16, batch index: 385, train loss: 1.322002\n",
      ">>> epoch: 16, batch index: 45, test loss: 1.623918, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 17, batch index: 385, train loss: 1.251863\n",
      ">>> epoch: 17, batch index: 45, test loss: 1.594565, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 18, batch index: 385, train loss: 1.281875\n",
      ">>> epoch: 18, batch index: 45, test loss: 1.604512, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 19, batch index: 385, train loss: 1.265247\n",
      ">>> epoch: 19, batch index: 45, test loss: 1.596008, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 20, batch index: 385, train loss: 1.224815\n",
      ">>> epoch: 20, batch index: 45, test loss: 1.601350, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 21, batch index: 385, train loss: 1.307015\n",
      ">>> epoch: 21, batch index: 45, test loss: 1.600757, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 22, batch index: 385, train loss: 1.329388\n",
      ">>> epoch: 22, batch index: 45, test loss: 1.601567, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 23, batch index: 385, train loss: 1.276398\n",
      ">>> epoch: 23, batch index: 45, test loss: 1.622651, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 24, batch index: 385, train loss: 1.273987\n",
      ">>> epoch: 24, batch index: 45, test loss: 1.603500, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 25, batch index: 385, train loss: 1.189979\n",
      ">>> epoch: 25, batch index: 45, test loss: 1.611815, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 26, batch index: 385, train loss: 1.289534\n",
      ">>> epoch: 26, batch index: 45, test loss: 1.611610, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 27, batch index: 385, train loss: 1.368933\n",
      ">>> epoch: 27, batch index: 45, test loss: 1.625932, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 28, batch index: 385, train loss: 1.321768\n",
      ">>> epoch: 28, batch index: 45, test loss: 1.628094, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 29, batch index: 385, train loss: 1.188639\n",
      ">>> epoch: 29, batch index: 45, test loss: 1.601443, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 30, batch index: 385, train loss: 1.237602\n",
      ">>> epoch: 30, batch index: 45, test loss: 1.603703, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 31, batch index: 385, train loss: 1.224724\n",
      ">>> epoch: 31, batch index: 45, test loss: 1.599875, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 32, batch index: 385, train loss: 1.225770\n",
      ">>> epoch: 32, batch index: 45, test loss: 1.605896, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 33, batch index: 385, train loss: 1.225813\n",
      ">>> epoch: 33, batch index: 45, test loss: 1.603717, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 34, batch index: 385, train loss: 1.264186\n",
      ">>> epoch: 34, batch index: 45, test loss: 1.620564, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 35, batch index: 385, train loss: 1.214037\n",
      ">>> epoch: 35, batch index: 45, test loss: 1.604305, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 36, batch index: 385, train loss: 1.249760\n",
      ">>> epoch: 36, batch index: 45, test loss: 1.601943, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 37, batch index: 385, train loss: 1.250211\n",
      ">>> epoch: 37, batch index: 45, test loss: 1.616518, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 38, batch index: 385, train loss: 1.237829\n",
      ">>> epoch: 38, batch index: 45, test loss: 1.613312, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 39, batch index: 385, train loss: 1.265943\n",
      ">>> epoch: 39, batch index: 45, test loss: 1.617025, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 40, batch index: 385, train loss: 1.216562\n",
      ">>> epoch: 40, batch index: 45, test loss: 1.616718, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 41, batch index: 385, train loss: 1.345749\n",
      ">>> epoch: 41, batch index: 45, test loss: 1.609865, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 42, batch index: 385, train loss: 1.250607\n",
      ">>> epoch: 42, batch index: 45, test loss: 1.598358, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 43, batch index: 385, train loss: 1.276491\n",
      ">>> epoch: 43, batch index: 45, test loss: 1.612032, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 44, batch index: 385, train loss: 1.284805\n",
      ">>> epoch: 44, batch index: 45, test loss: 1.603524, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 45, batch index: 385, train loss: 1.237013\n",
      ">>> epoch: 45, batch index: 45, test loss: 1.608563, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 46, batch index: 385, train loss: 1.249638\n",
      ">>> epoch: 46, batch index: 45, test loss: 1.609492, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 47, batch index: 385, train loss: 1.299382\n",
      ">>> epoch: 47, batch index: 45, test loss: 1.620015, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 48, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 48, batch index: 45, test loss: 1.609152, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 49, batch index: 385, train loss: 1.261564\n",
      ">>> epoch: 49, batch index: 45, test loss: 1.606698, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 50, batch index: 385, train loss: 1.201015\n",
      ">>> epoch: 50, batch index: 45, test loss: 1.604484, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 51, batch index: 385, train loss: 1.213636\n",
      ">>> epoch: 51, batch index: 45, test loss: 1.606068, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 52, batch index: 385, train loss: 1.202963\n",
      ">>> epoch: 52, batch index: 45, test loss: 1.606053, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 53, batch index: 385, train loss: 1.249183\n",
      ">>> epoch: 53, batch index: 45, test loss: 1.608846, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 54, batch index: 385, train loss: 1.248933\n",
      ">>> epoch: 54, batch index: 45, test loss: 1.613479, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 55, batch index: 385, train loss: 1.249711\n",
      ">>> epoch: 55, batch index: 45, test loss: 1.636977, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 56, batch index: 385, train loss: 1.207113\n",
      ">>> epoch: 56, batch index: 45, test loss: 1.610556, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 57, batch index: 385, train loss: 1.202256\n",
      ">>> epoch: 57, batch index: 45, test loss: 1.600623, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 58, batch index: 385, train loss: 1.285151\n",
      ">>> epoch: 58, batch index: 45, test loss: 1.601629, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 59, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 59, batch index: 45, test loss: 1.616421, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 60, batch index: 385, train loss: 1.225628\n",
      ">>> epoch: 60, batch index: 45, test loss: 1.620480, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 61, batch index: 385, train loss: 1.225321\n",
      ">>> epoch: 61, batch index: 45, test loss: 1.624354, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 62, batch index: 385, train loss: 1.261711\n",
      ">>> epoch: 62, batch index: 45, test loss: 1.601771, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 63, batch index: 385, train loss: 1.260752\n",
      ">>> epoch: 63, batch index: 45, test loss: 1.600646, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 64, batch index: 385, train loss: 1.201635\n",
      ">>> epoch: 64, batch index: 45, test loss: 1.607955, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 65, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 65, batch index: 45, test loss: 1.619668, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 66, batch index: 385, train loss: 1.225099\n",
      ">>> epoch: 66, batch index: 45, test loss: 1.597951, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 67, batch index: 385, train loss: 1.248723\n",
      ">>> epoch: 67, batch index: 45, test loss: 1.611385, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 68, batch index: 385, train loss: 1.220961\n",
      ">>> epoch: 68, batch index: 45, test loss: 1.600417, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 69, batch index: 385, train loss: 1.237727\n",
      ">>> epoch: 69, batch index: 45, test loss: 1.620497, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 70, batch index: 385, train loss: 1.259616\n",
      ">>> epoch: 70, batch index: 45, test loss: 1.611051, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 71, batch index: 385, train loss: 1.224841\n",
      ">>> epoch: 71, batch index: 45, test loss: 1.611974, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 72, batch index: 385, train loss: 1.226789\n",
      ">>> epoch: 72, batch index: 45, test loss: 1.617792, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 73, batch index: 385, train loss: 1.224761\n",
      ">>> epoch: 73, batch index: 45, test loss: 1.598022, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 74, batch index: 385, train loss: 1.237076\n",
      ">>> epoch: 74, batch index: 45, test loss: 1.615629, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 75, batch index: 385, train loss: 1.180207\n",
      ">>> epoch: 75, batch index: 45, test loss: 1.616053, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 76, batch index: 385, train loss: 1.200979\n",
      ">>> epoch: 76, batch index: 45, test loss: 1.625769, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 77, batch index: 385, train loss: 1.189517\n",
      ">>> epoch: 77, batch index: 45, test loss: 1.610007, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 78, batch index: 385, train loss: 1.261816\n",
      ">>> epoch: 78, batch index: 45, test loss: 1.612649, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 79, batch index: 385, train loss: 1.209001\n",
      ">>> epoch: 79, batch index: 45, test loss: 1.613728, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 80, batch index: 385, train loss: 1.214731\n",
      ">>> epoch: 80, batch index: 45, test loss: 1.625728, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 81, batch index: 385, train loss: 1.261430\n",
      ">>> epoch: 81, batch index: 45, test loss: 1.616645, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 82, batch index: 385, train loss: 1.238518\n",
      ">>> epoch: 82, batch index: 45, test loss: 1.607620, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 83, batch index: 385, train loss: 1.297806\n",
      ">>> epoch: 83, batch index: 45, test loss: 1.608844, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 84, batch index: 385, train loss: 1.212936\n",
      ">>> epoch: 84, batch index: 45, test loss: 1.619726, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 85, batch index: 385, train loss: 1.235307\n",
      ">>> epoch: 85, batch index: 45, test loss: 1.609901, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 86, batch index: 385, train loss: 1.237708\n",
      ">>> epoch: 86, batch index: 45, test loss: 1.620856, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 87, batch index: 385, train loss: 1.237814\n",
      ">>> epoch: 87, batch index: 45, test loss: 1.621713, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 88, batch index: 385, train loss: 1.178741\n",
      ">>> epoch: 88, batch index: 45, test loss: 1.628344, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 89, batch index: 385, train loss: 1.275209\n",
      ">>> epoch: 89, batch index: 45, test loss: 1.629444, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 90, batch index: 385, train loss: 1.178670\n",
      ">>> epoch: 90, batch index: 45, test loss: 1.630979, acc: 0.521\n",
      "==================================================================\n",
      ">>> epoch: 91, batch index: 385, train loss: 1.246612\n",
      ">>> epoch: 91, batch index: 45, test loss: 1.621270, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 92, batch index: 385, train loss: 1.218285\n",
      ">>> epoch: 92, batch index: 45, test loss: 1.618983, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 93, batch index: 385, train loss: 1.239455\n",
      ">>> epoch: 93, batch index: 45, test loss: 1.624345, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 94, batch index: 385, train loss: 1.203081\n",
      ">>> epoch: 94, batch index: 45, test loss: 1.627695, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 95, batch index: 385, train loss: 1.249933\n",
      ">>> epoch: 95, batch index: 45, test loss: 1.625268, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 96, batch index: 385, train loss: 1.237739\n",
      ">>> epoch: 96, batch index: 45, test loss: 1.616641, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 97, batch index: 385, train loss: 1.225817\n",
      ">>> epoch: 97, batch index: 45, test loss: 1.620584, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 98, batch index: 385, train loss: 1.273204\n",
      ">>> epoch: 98, batch index: 45, test loss: 1.604816, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 99, batch index: 385, train loss: 1.261909\n",
      ">>> epoch: 99, batch index: 45, test loss: 1.633628, acc: 0.521\n",
      "==================================================================\n",
      ">>> epoch: 100, batch index: 385, train loss: 1.249781\n",
      ">>> epoch: 100, batch index: 45, test loss: 1.611062, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 101, batch index: 385, train loss: 1.201694\n",
      ">>> epoch: 101, batch index: 45, test loss: 1.619809, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 102, batch index: 385, train loss: 1.294186\n",
      ">>> epoch: 102, batch index: 45, test loss: 1.610226, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 103, batch index: 385, train loss: 1.201924\n",
      ">>> epoch: 103, batch index: 45, test loss: 1.614973, acc: 0.520\n",
      "==================================================================\n",
      ">>> epoch: 104, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 104, batch index: 45, test loss: 1.622250, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 105, batch index: 385, train loss: 1.225809\n",
      ">>> epoch: 105, batch index: 45, test loss: 1.610479, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 106, batch index: 385, train loss: 1.189794\n",
      ">>> epoch: 106, batch index: 45, test loss: 1.612403, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 107, batch index: 385, train loss: 1.203018\n",
      ">>> epoch: 107, batch index: 45, test loss: 1.616216, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 108, batch index: 385, train loss: 1.242951\n",
      ">>> epoch: 108, batch index: 45, test loss: 1.603397, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 109, batch index: 385, train loss: 1.201576\n",
      ">>> epoch: 109, batch index: 45, test loss: 1.603762, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 110, batch index: 385, train loss: 1.224978\n",
      ">>> epoch: 110, batch index: 45, test loss: 1.610938, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 111, batch index: 385, train loss: 1.249765\n",
      ">>> epoch: 111, batch index: 45, test loss: 1.599812, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 112, batch index: 385, train loss: 1.261036\n",
      ">>> epoch: 112, batch index: 45, test loss: 1.627141, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 113, batch index: 385, train loss: 1.249090\n",
      ">>> epoch: 113, batch index: 45, test loss: 1.617191, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 114, batch index: 385, train loss: 1.235151\n",
      ">>> epoch: 114, batch index: 45, test loss: 1.616401, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 115, batch index: 385, train loss: 1.260996\n",
      ">>> epoch: 115, batch index: 45, test loss: 1.610886, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 116, batch index: 385, train loss: 1.244887\n",
      ">>> epoch: 116, batch index: 45, test loss: 1.619669, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 117, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 117, batch index: 45, test loss: 1.607000, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 118, batch index: 385, train loss: 1.212443\n",
      ">>> epoch: 118, batch index: 45, test loss: 1.610626, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 119, batch index: 385, train loss: 1.205186\n",
      ">>> epoch: 119, batch index: 45, test loss: 1.619638, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 120, batch index: 385, train loss: 1.214123\n",
      ">>> epoch: 120, batch index: 45, test loss: 1.627937, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 121, batch index: 385, train loss: 1.196414\n",
      ">>> epoch: 121, batch index: 45, test loss: 1.603309, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 122, batch index: 385, train loss: 1.284681\n",
      ">>> epoch: 122, batch index: 45, test loss: 1.618229, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 123, batch index: 385, train loss: 1.223313\n",
      ">>> epoch: 123, batch index: 45, test loss: 1.617769, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 124, batch index: 385, train loss: 1.304692\n",
      ">>> epoch: 124, batch index: 45, test loss: 1.623709, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 125, batch index: 385, train loss: 1.201576\n",
      ">>> epoch: 125, batch index: 45, test loss: 1.606304, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 126, batch index: 385, train loss: 1.310160\n",
      ">>> epoch: 126, batch index: 45, test loss: 1.620439, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 127, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 127, batch index: 45, test loss: 1.621473, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 128, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 128, batch index: 45, test loss: 1.606467, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 129, batch index: 385, train loss: 1.201577\n",
      ">>> epoch: 129, batch index: 45, test loss: 1.623787, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 130, batch index: 385, train loss: 1.249762\n",
      ">>> epoch: 130, batch index: 45, test loss: 1.617633, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 131, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 131, batch index: 45, test loss: 1.624457, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 132, batch index: 385, train loss: 1.226107\n",
      ">>> epoch: 132, batch index: 45, test loss: 1.614447, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 133, batch index: 385, train loss: 1.213692\n",
      ">>> epoch: 133, batch index: 45, test loss: 1.620937, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 134, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 134, batch index: 45, test loss: 1.620015, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 135, batch index: 385, train loss: 1.201570\n",
      ">>> epoch: 135, batch index: 45, test loss: 1.607923, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 136, batch index: 385, train loss: 1.213794\n",
      ">>> epoch: 136, batch index: 45, test loss: 1.628227, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 137, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 137, batch index: 45, test loss: 1.619398, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 138, batch index: 385, train loss: 1.226618\n",
      ">>> epoch: 138, batch index: 45, test loss: 1.618528, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 139, batch index: 385, train loss: 1.238618\n",
      ">>> epoch: 139, batch index: 45, test loss: 1.609239, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 140, batch index: 385, train loss: 1.237060\n",
      ">>> epoch: 140, batch index: 45, test loss: 1.605829, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 141, batch index: 385, train loss: 1.266984\n",
      ">>> epoch: 141, batch index: 45, test loss: 1.611881, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 142, batch index: 385, train loss: 1.225482\n",
      ">>> epoch: 142, batch index: 45, test loss: 1.626613, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 143, batch index: 385, train loss: 1.201902\n",
      ">>> epoch: 143, batch index: 45, test loss: 1.628058, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 144, batch index: 385, train loss: 1.189660\n",
      ">>> epoch: 144, batch index: 45, test loss: 1.623865, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 145, batch index: 385, train loss: 1.225683\n",
      ">>> epoch: 145, batch index: 45, test loss: 1.617363, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 146, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 146, batch index: 45, test loss: 1.615080, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 147, batch index: 385, train loss: 1.213901\n",
      ">>> epoch: 147, batch index: 45, test loss: 1.632337, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 148, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 148, batch index: 45, test loss: 1.620142, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 149, batch index: 385, train loss: 1.201701\n",
      ">>> epoch: 149, batch index: 45, test loss: 1.627896, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 150, batch index: 385, train loss: 1.189564\n",
      ">>> epoch: 150, batch index: 45, test loss: 1.615317, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 151, batch index: 385, train loss: 1.225677\n",
      ">>> epoch: 151, batch index: 45, test loss: 1.625122, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 152, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 152, batch index: 45, test loss: 1.608677, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 153, batch index: 385, train loss: 1.201573\n",
      ">>> epoch: 153, batch index: 45, test loss: 1.620148, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 154, batch index: 385, train loss: 1.249647\n",
      ">>> epoch: 154, batch index: 45, test loss: 1.625252, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 155, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 155, batch index: 45, test loss: 1.610595, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 156, batch index: 385, train loss: 1.195067\n",
      ">>> epoch: 156, batch index: 45, test loss: 1.609348, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 157, batch index: 385, train loss: 1.261807\n",
      ">>> epoch: 157, batch index: 45, test loss: 1.607771, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 158, batch index: 385, train loss: 1.249271\n",
      ">>> epoch: 158, batch index: 45, test loss: 1.624841, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 159, batch index: 385, train loss: 1.212299\n",
      ">>> epoch: 159, batch index: 45, test loss: 1.608192, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 160, batch index: 385, train loss: 1.225078\n",
      ">>> epoch: 160, batch index: 45, test loss: 1.612275, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 161, batch index: 385, train loss: 1.189671\n",
      ">>> epoch: 161, batch index: 45, test loss: 1.605439, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 162, batch index: 385, train loss: 1.225725\n",
      ">>> epoch: 162, batch index: 45, test loss: 1.619380, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 163, batch index: 385, train loss: 1.237706\n",
      ">>> epoch: 163, batch index: 45, test loss: 1.605389, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 164, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 164, batch index: 45, test loss: 1.619843, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 165, batch index: 385, train loss: 1.225666\n",
      ">>> epoch: 165, batch index: 45, test loss: 1.614730, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 166, batch index: 385, train loss: 1.272923\n",
      ">>> epoch: 166, batch index: 45, test loss: 1.613167, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 167, batch index: 385, train loss: 1.211043\n",
      ">>> epoch: 167, batch index: 45, test loss: 1.626210, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 168, batch index: 385, train loss: 1.201590\n",
      ">>> epoch: 168, batch index: 45, test loss: 1.624875, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 169, batch index: 385, train loss: 1.213662\n",
      ">>> epoch: 169, batch index: 45, test loss: 1.613278, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 170, batch index: 385, train loss: 1.225711\n",
      ">>> epoch: 170, batch index: 45, test loss: 1.631473, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 171, batch index: 385, train loss: 1.237717\n",
      ">>> epoch: 171, batch index: 45, test loss: 1.607476, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 172, batch index: 385, train loss: 1.215784\n",
      ">>> epoch: 172, batch index: 45, test loss: 1.606083, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 173, batch index: 385, train loss: 1.261075\n",
      ">>> epoch: 173, batch index: 45, test loss: 1.609161, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 174, batch index: 385, train loss: 1.247732\n",
      ">>> epoch: 174, batch index: 45, test loss: 1.602790, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 175, batch index: 385, train loss: 1.237057\n",
      ">>> epoch: 175, batch index: 45, test loss: 1.595387, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 176, batch index: 385, train loss: 1.249778\n",
      ">>> epoch: 176, batch index: 45, test loss: 1.627010, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 177, batch index: 385, train loss: 1.189522\n",
      ">>> epoch: 177, batch index: 45, test loss: 1.622223, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 178, batch index: 385, train loss: 1.261809\n",
      ">>> epoch: 178, batch index: 45, test loss: 1.625115, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 179, batch index: 385, train loss: 1.201259\n",
      ">>> epoch: 179, batch index: 45, test loss: 1.600254, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 180, batch index: 385, train loss: 1.225217\n",
      ">>> epoch: 180, batch index: 45, test loss: 1.625109, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 181, batch index: 385, train loss: 1.225486\n",
      ">>> epoch: 181, batch index: 45, test loss: 1.615913, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 182, batch index: 385, train loss: 1.249853\n",
      ">>> epoch: 182, batch index: 45, test loss: 1.627126, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 183, batch index: 385, train loss: 1.213638\n",
      ">>> epoch: 183, batch index: 45, test loss: 1.619014, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 184, batch index: 385, train loss: 1.213947\n",
      ">>> epoch: 184, batch index: 45, test loss: 1.610122, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 185, batch index: 385, train loss: 1.203227\n",
      ">>> epoch: 185, batch index: 45, test loss: 1.607325, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 186, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 186, batch index: 45, test loss: 1.622954, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 187, batch index: 385, train loss: 1.285781\n",
      ">>> epoch: 187, batch index: 45, test loss: 1.625355, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 188, batch index: 385, train loss: 1.225706\n",
      ">>> epoch: 188, batch index: 45, test loss: 1.611007, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 189, batch index: 385, train loss: 1.177479\n",
      ">>> epoch: 189, batch index: 45, test loss: 1.614269, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 190, batch index: 385, train loss: 1.237362\n",
      ">>> epoch: 190, batch index: 45, test loss: 1.617652, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 191, batch index: 385, train loss: 1.274190\n",
      ">>> epoch: 191, batch index: 45, test loss: 1.619547, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 192, batch index: 385, train loss: 1.201598\n",
      ">>> epoch: 192, batch index: 45, test loss: 1.632011, acc: 0.520\n",
      "==================================================================\n",
      ">>> epoch: 193, batch index: 385, train loss: 1.201581\n",
      ">>> epoch: 193, batch index: 45, test loss: 1.625963, acc: 0.519\n",
      "==================================================================\n",
      ">>> epoch: 194, batch index: 385, train loss: 1.237719\n",
      ">>> epoch: 194, batch index: 45, test loss: 1.630692, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 195, batch index: 385, train loss: 1.249281\n",
      ">>> epoch: 195, batch index: 45, test loss: 1.625146, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 196, batch index: 385, train loss: 1.250048\n",
      ">>> epoch: 196, batch index: 45, test loss: 1.615520, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 197, batch index: 385, train loss: 1.225662\n",
      ">>> epoch: 197, batch index: 45, test loss: 1.613161, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 198, batch index: 385, train loss: 1.225667\n",
      ">>> epoch: 198, batch index: 45, test loss: 1.621362, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 199, batch index: 385, train loss: 1.213764\n",
      ">>> epoch: 199, batch index: 45, test loss: 1.605193, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 200, batch index: 385, train loss: 1.177621\n",
      ">>> epoch: 200, batch index: 45, test loss: 1.605268, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 201, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 201, batch index: 45, test loss: 1.631511, acc: 0.519\n",
      "==================================================================\n",
      ">>> epoch: 202, batch index: 385, train loss: 1.220029\n",
      ">>> epoch: 202, batch index: 45, test loss: 1.626539, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 203, batch index: 385, train loss: 1.213650\n",
      ">>> epoch: 203, batch index: 45, test loss: 1.637462, acc: 0.518\n",
      "==================================================================\n",
      ">>> epoch: 204, batch index: 385, train loss: 1.250350\n",
      ">>> epoch: 204, batch index: 45, test loss: 1.637595, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 205, batch index: 385, train loss: 1.249253\n",
      ">>> epoch: 205, batch index: 45, test loss: 1.607863, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 206, batch index: 385, train loss: 1.189526\n",
      ">>> epoch: 206, batch index: 45, test loss: 1.621793, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 207, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 207, batch index: 45, test loss: 1.611366, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 208, batch index: 385, train loss: 1.261265\n",
      ">>> epoch: 208, batch index: 45, test loss: 1.621570, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 209, batch index: 385, train loss: 1.225741\n",
      ">>> epoch: 209, batch index: 45, test loss: 1.615809, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 210, batch index: 385, train loss: 1.214510\n",
      ">>> epoch: 210, batch index: 45, test loss: 1.627207, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 211, batch index: 385, train loss: 1.213532\n",
      ">>> epoch: 211, batch index: 45, test loss: 1.621768, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 212, batch index: 385, train loss: 1.177518\n",
      ">>> epoch: 212, batch index: 45, test loss: 1.610241, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 213, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 213, batch index: 45, test loss: 1.595582, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 214, batch index: 385, train loss: 1.225740\n",
      ">>> epoch: 214, batch index: 45, test loss: 1.618160, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 215, batch index: 385, train loss: 1.189621\n",
      ">>> epoch: 215, batch index: 45, test loss: 1.620842, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 216, batch index: 385, train loss: 1.213625\n",
      ">>> epoch: 216, batch index: 45, test loss: 1.619961, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 217, batch index: 385, train loss: 1.250491\n",
      ">>> epoch: 217, batch index: 45, test loss: 1.617952, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 218, batch index: 385, train loss: 1.213620\n",
      ">>> epoch: 218, batch index: 45, test loss: 1.605990, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 219, batch index: 385, train loss: 1.225473\n",
      ">>> epoch: 219, batch index: 45, test loss: 1.624831, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 220, batch index: 385, train loss: 1.188789\n",
      ">>> epoch: 220, batch index: 45, test loss: 1.614074, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 221, batch index: 385, train loss: 1.189598\n",
      ">>> epoch: 221, batch index: 45, test loss: 1.612969, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 222, batch index: 385, train loss: 1.201114\n",
      ">>> epoch: 222, batch index: 45, test loss: 1.616653, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 223, batch index: 385, train loss: 1.201736\n",
      ">>> epoch: 223, batch index: 45, test loss: 1.613202, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 224, batch index: 385, train loss: 1.247762\n",
      ">>> epoch: 224, batch index: 45, test loss: 1.613674, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 225, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 225, batch index: 45, test loss: 1.627475, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 226, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 226, batch index: 45, test loss: 1.622667, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 227, batch index: 385, train loss: 1.200942\n",
      ">>> epoch: 227, batch index: 45, test loss: 1.616366, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 228, batch index: 385, train loss: 1.213616\n",
      ">>> epoch: 228, batch index: 45, test loss: 1.628089, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 229, batch index: 385, train loss: 1.204524\n",
      ">>> epoch: 229, batch index: 45, test loss: 1.619424, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 230, batch index: 385, train loss: 1.201672\n",
      ">>> epoch: 230, batch index: 45, test loss: 1.601045, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 231, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 231, batch index: 45, test loss: 1.630451, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 232, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 232, batch index: 45, test loss: 1.617320, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 233, batch index: 385, train loss: 1.273857\n",
      ">>> epoch: 233, batch index: 45, test loss: 1.610806, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 234, batch index: 385, train loss: 1.190906\n",
      ">>> epoch: 234, batch index: 45, test loss: 1.634512, acc: 0.525\n",
      "==================================================================\n",
      ">>> epoch: 235, batch index: 385, train loss: 1.261807\n",
      ">>> epoch: 235, batch index: 45, test loss: 1.613070, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 236, batch index: 385, train loss: 1.237231\n",
      ">>> epoch: 236, batch index: 45, test loss: 1.611561, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 237, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 237, batch index: 45, test loss: 1.621660, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 238, batch index: 385, train loss: 1.225729\n",
      ">>> epoch: 238, batch index: 45, test loss: 1.616293, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 239, batch index: 385, train loss: 1.224971\n",
      ">>> epoch: 239, batch index: 45, test loss: 1.612815, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 240, batch index: 385, train loss: 1.249764\n",
      ">>> epoch: 240, batch index: 45, test loss: 1.621477, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 241, batch index: 385, train loss: 1.213621\n",
      ">>> epoch: 241, batch index: 45, test loss: 1.621388, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 242, batch index: 385, train loss: 1.237237\n",
      ">>> epoch: 242, batch index: 45, test loss: 1.617181, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 243, batch index: 385, train loss: 1.213617\n",
      ">>> epoch: 243, batch index: 45, test loss: 1.630960, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 244, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 244, batch index: 45, test loss: 1.619039, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 245, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 245, batch index: 45, test loss: 1.626508, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 246, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 246, batch index: 45, test loss: 1.613533, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 247, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 247, batch index: 45, test loss: 1.608386, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 248, batch index: 385, train loss: 1.201618\n",
      ">>> epoch: 248, batch index: 45, test loss: 1.621746, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 249, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 249, batch index: 45, test loss: 1.628897, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 250, batch index: 385, train loss: 1.201471\n",
      ">>> epoch: 250, batch index: 45, test loss: 1.609662, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 251, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 251, batch index: 45, test loss: 1.613144, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 252, batch index: 385, train loss: 1.224678\n",
      ">>> epoch: 252, batch index: 45, test loss: 1.620841, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 253, batch index: 385, train loss: 1.202988\n",
      ">>> epoch: 253, batch index: 45, test loss: 1.620966, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 254, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 254, batch index: 45, test loss: 1.607082, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 255, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 255, batch index: 45, test loss: 1.619436, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 256, batch index: 385, train loss: 1.213671\n",
      ">>> epoch: 256, batch index: 45, test loss: 1.611875, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 257, batch index: 385, train loss: 1.225673\n",
      ">>> epoch: 257, batch index: 45, test loss: 1.599718, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 258, batch index: 385, train loss: 1.214671\n",
      ">>> epoch: 258, batch index: 45, test loss: 1.598349, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 259, batch index: 385, train loss: 1.198212\n",
      ">>> epoch: 259, batch index: 45, test loss: 1.605305, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 260, batch index: 385, train loss: 1.213632\n",
      ">>> epoch: 260, batch index: 45, test loss: 1.614517, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 261, batch index: 385, train loss: 1.213649\n",
      ">>> epoch: 261, batch index: 45, test loss: 1.604896, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 262, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 262, batch index: 45, test loss: 1.620953, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 263, batch index: 385, train loss: 1.224964\n",
      ">>> epoch: 263, batch index: 45, test loss: 1.614637, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 264, batch index: 385, train loss: 1.213616\n",
      ">>> epoch: 264, batch index: 45, test loss: 1.615999, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 265, batch index: 385, train loss: 1.165432\n",
      ">>> epoch: 265, batch index: 45, test loss: 1.614435, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 266, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 266, batch index: 45, test loss: 1.624250, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 267, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 267, batch index: 45, test loss: 1.614771, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 268, batch index: 385, train loss: 1.213270\n",
      ">>> epoch: 268, batch index: 45, test loss: 1.622377, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 269, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 269, batch index: 45, test loss: 1.605005, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 270, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 270, batch index: 45, test loss: 1.634464, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 271, batch index: 385, train loss: 1.237506\n",
      ">>> epoch: 271, batch index: 45, test loss: 1.599653, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 272, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 272, batch index: 45, test loss: 1.623034, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 273, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 273, batch index: 45, test loss: 1.616915, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 274, batch index: 385, train loss: 1.189521\n",
      ">>> epoch: 274, batch index: 45, test loss: 1.617233, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 275, batch index: 385, train loss: 1.201588\n",
      ">>> epoch: 275, batch index: 45, test loss: 1.606797, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 276, batch index: 385, train loss: 1.225675\n",
      ">>> epoch: 276, batch index: 45, test loss: 1.601479, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 277, batch index: 385, train loss: 1.237722\n",
      ">>> epoch: 277, batch index: 45, test loss: 1.623098, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 278, batch index: 385, train loss: 1.189521\n",
      ">>> epoch: 278, batch index: 45, test loss: 1.616684, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 279, batch index: 385, train loss: 1.224897\n",
      ">>> epoch: 279, batch index: 45, test loss: 1.613327, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 280, batch index: 385, train loss: 1.201622\n",
      ">>> epoch: 280, batch index: 45, test loss: 1.616907, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 281, batch index: 385, train loss: 1.188885\n",
      ">>> epoch: 281, batch index: 45, test loss: 1.613828, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 282, batch index: 385, train loss: 1.213823\n",
      ">>> epoch: 282, batch index: 45, test loss: 1.629290, acc: 0.524\n",
      "==================================================================\n",
      ">>> epoch: 283, batch index: 385, train loss: 1.188991\n",
      ">>> epoch: 283, batch index: 45, test loss: 1.625894, acc: 0.526\n",
      "==================================================================\n",
      ">>> epoch: 284, batch index: 385, train loss: 1.225665\n",
      ">>> epoch: 284, batch index: 45, test loss: 1.627470, acc: 0.519\n",
      "==================================================================\n",
      ">>> epoch: 285, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 285, batch index: 45, test loss: 1.628499, acc: 0.523\n",
      "==================================================================\n",
      ">>> epoch: 286, batch index: 385, train loss: 1.201571\n",
      ">>> epoch: 286, batch index: 45, test loss: 1.608548, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 287, batch index: 385, train loss: 1.211809\n",
      ">>> epoch: 287, batch index: 45, test loss: 1.607688, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 288, batch index: 385, train loss: 1.226102\n",
      ">>> epoch: 288, batch index: 45, test loss: 1.611196, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 289, batch index: 385, train loss: 1.214000\n",
      ">>> epoch: 289, batch index: 45, test loss: 1.634189, acc: 0.522\n",
      "==================================================================\n",
      ">>> epoch: 290, batch index: 385, train loss: 1.229191\n",
      ">>> epoch: 290, batch index: 45, test loss: 1.614494, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 291, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 291, batch index: 45, test loss: 1.600162, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 292, batch index: 385, train loss: 1.224798\n",
      ">>> epoch: 292, batch index: 45, test loss: 1.611001, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 293, batch index: 385, train loss: 1.237721\n",
      ">>> epoch: 293, batch index: 45, test loss: 1.608975, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 294, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 294, batch index: 45, test loss: 1.610286, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 295, batch index: 385, train loss: 1.272399\n",
      ">>> epoch: 295, batch index: 45, test loss: 1.613593, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 296, batch index: 385, train loss: 1.213617\n",
      ">>> epoch: 296, batch index: 45, test loss: 1.611573, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 297, batch index: 385, train loss: 1.189526\n",
      ">>> epoch: 297, batch index: 45, test loss: 1.629178, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 298, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 298, batch index: 45, test loss: 1.606616, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 299, batch index: 385, train loss: 1.236953\n",
      ">>> epoch: 299, batch index: 45, test loss: 1.597811, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 300, batch index: 385, train loss: 1.237713\n",
      ">>> epoch: 300, batch index: 45, test loss: 1.609617, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 301, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 301, batch index: 45, test loss: 1.611112, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 302, batch index: 385, train loss: 1.249018\n",
      ">>> epoch: 302, batch index: 45, test loss: 1.606723, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 303, batch index: 385, train loss: 1.201572\n",
      ">>> epoch: 303, batch index: 45, test loss: 1.615862, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 304, batch index: 385, train loss: 1.261778\n",
      ">>> epoch: 304, batch index: 45, test loss: 1.602850, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 305, batch index: 385, train loss: 1.177541\n",
      ">>> epoch: 305, batch index: 45, test loss: 1.629273, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 306, batch index: 385, train loss: 1.189522\n",
      ">>> epoch: 306, batch index: 45, test loss: 1.609512, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 307, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 307, batch index: 45, test loss: 1.621212, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 308, batch index: 385, train loss: 1.225205\n",
      ">>> epoch: 308, batch index: 45, test loss: 1.617580, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 309, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 309, batch index: 45, test loss: 1.612883, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 310, batch index: 385, train loss: 1.212857\n",
      ">>> epoch: 310, batch index: 45, test loss: 1.606571, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 311, batch index: 385, train loss: 1.237733\n",
      ">>> epoch: 311, batch index: 45, test loss: 1.610584, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 312, batch index: 385, train loss: 1.189526\n",
      ">>> epoch: 312, batch index: 45, test loss: 1.611360, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 313, batch index: 385, train loss: 1.249305\n",
      ">>> epoch: 313, batch index: 45, test loss: 1.614881, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 314, batch index: 385, train loss: 1.213617\n",
      ">>> epoch: 314, batch index: 45, test loss: 1.610976, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 315, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 315, batch index: 45, test loss: 1.607075, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 316, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 316, batch index: 45, test loss: 1.624990, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 317, batch index: 385, train loss: 1.177474\n",
      ">>> epoch: 317, batch index: 45, test loss: 1.614465, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 318, batch index: 385, train loss: 1.201576\n",
      ">>> epoch: 318, batch index: 45, test loss: 1.609868, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 319, batch index: 385, train loss: 1.201581\n",
      ">>> epoch: 319, batch index: 45, test loss: 1.626123, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 320, batch index: 385, train loss: 1.189748\n",
      ">>> epoch: 320, batch index: 45, test loss: 1.616316, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 321, batch index: 385, train loss: 1.213596\n",
      ">>> epoch: 321, batch index: 45, test loss: 1.621647, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 322, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 322, batch index: 45, test loss: 1.619260, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 323, batch index: 385, train loss: 1.225679\n",
      ">>> epoch: 323, batch index: 45, test loss: 1.620590, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 324, batch index: 385, train loss: 1.201054\n",
      ">>> epoch: 324, batch index: 45, test loss: 1.613265, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 325, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 325, batch index: 45, test loss: 1.619711, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 326, batch index: 385, train loss: 1.237712\n",
      ">>> epoch: 326, batch index: 45, test loss: 1.608450, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 327, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 327, batch index: 45, test loss: 1.621216, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 328, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 328, batch index: 45, test loss: 1.613624, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 329, batch index: 385, train loss: 1.177455\n",
      ">>> epoch: 329, batch index: 45, test loss: 1.629653, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 330, batch index: 385, train loss: 1.177473\n",
      ">>> epoch: 330, batch index: 45, test loss: 1.619255, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 331, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 331, batch index: 45, test loss: 1.615837, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 332, batch index: 385, train loss: 1.201297\n",
      ">>> epoch: 332, batch index: 45, test loss: 1.618329, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 333, batch index: 385, train loss: 1.201579\n",
      ">>> epoch: 333, batch index: 45, test loss: 1.623381, acc: 0.527\n",
      "==================================================================\n",
      ">>> epoch: 334, batch index: 385, train loss: 1.213288\n",
      ">>> epoch: 334, batch index: 45, test loss: 1.631097, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 335, batch index: 385, train loss: 1.201578\n",
      ">>> epoch: 335, batch index: 45, test loss: 1.606914, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 336, batch index: 385, train loss: 1.224825\n",
      ">>> epoch: 336, batch index: 45, test loss: 1.610434, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 337, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 337, batch index: 45, test loss: 1.612636, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 338, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 338, batch index: 45, test loss: 1.613306, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 339, batch index: 385, train loss: 1.261810\n",
      ">>> epoch: 339, batch index: 45, test loss: 1.613867, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 340, batch index: 385, train loss: 1.213824\n",
      ">>> epoch: 340, batch index: 45, test loss: 1.626264, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 341, batch index: 385, train loss: 1.236572\n",
      ">>> epoch: 341, batch index: 45, test loss: 1.606184, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 342, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 342, batch index: 45, test loss: 1.620090, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 343, batch index: 385, train loss: 1.273855\n",
      ">>> epoch: 343, batch index: 45, test loss: 1.605560, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 344, batch index: 385, train loss: 1.201565\n",
      ">>> epoch: 344, batch index: 45, test loss: 1.616848, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 345, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 345, batch index: 45, test loss: 1.613261, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 346, batch index: 385, train loss: 1.212976\n",
      ">>> epoch: 346, batch index: 45, test loss: 1.612153, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 347, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 347, batch index: 45, test loss: 1.619454, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 348, batch index: 385, train loss: 1.213623\n",
      ">>> epoch: 348, batch index: 45, test loss: 1.616744, acc: 0.528\n",
      "==================================================================\n",
      ">>> epoch: 349, batch index: 385, train loss: 1.261268\n",
      ">>> epoch: 349, batch index: 45, test loss: 1.604312, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 350, batch index: 385, train loss: 1.248802\n",
      ">>> epoch: 350, batch index: 45, test loss: 1.613044, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 351, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 351, batch index: 45, test loss: 1.619087, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 352, batch index: 385, train loss: 1.189352\n",
      ">>> epoch: 352, batch index: 45, test loss: 1.619615, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 353, batch index: 385, train loss: 1.177490\n",
      ">>> epoch: 353, batch index: 45, test loss: 1.626522, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 354, batch index: 385, train loss: 1.176759\n",
      ">>> epoch: 354, batch index: 45, test loss: 1.618616, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 355, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 355, batch index: 45, test loss: 1.631318, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 356, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 356, batch index: 45, test loss: 1.597961, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 357, batch index: 385, train loss: 1.188835\n",
      ">>> epoch: 357, batch index: 45, test loss: 1.608187, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 358, batch index: 385, train loss: 1.224867\n",
      ">>> epoch: 358, batch index: 45, test loss: 1.586029, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 359, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 359, batch index: 45, test loss: 1.597017, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 360, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 360, batch index: 45, test loss: 1.612760, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 361, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 361, batch index: 45, test loss: 1.613632, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 362, batch index: 385, train loss: 1.213618\n",
      ">>> epoch: 362, batch index: 45, test loss: 1.618705, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 363, batch index: 385, train loss: 1.249757\n",
      ">>> epoch: 363, batch index: 45, test loss: 1.606668, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 364, batch index: 385, train loss: 1.177486\n",
      ">>> epoch: 364, batch index: 45, test loss: 1.618493, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 365, batch index: 385, train loss: 1.237716\n",
      ">>> epoch: 365, batch index: 45, test loss: 1.597008, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 366, batch index: 385, train loss: 1.190210\n",
      ">>> epoch: 366, batch index: 45, test loss: 1.621391, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 367, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 367, batch index: 45, test loss: 1.620037, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 368, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 368, batch index: 45, test loss: 1.620428, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 369, batch index: 385, train loss: 1.188945\n",
      ">>> epoch: 369, batch index: 45, test loss: 1.607412, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 370, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 370, batch index: 45, test loss: 1.608419, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 371, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 371, batch index: 45, test loss: 1.606282, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 372, batch index: 385, train loss: 1.261476\n",
      ">>> epoch: 372, batch index: 45, test loss: 1.593333, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 373, batch index: 385, train loss: 1.201679\n",
      ">>> epoch: 373, batch index: 45, test loss: 1.623761, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 374, batch index: 385, train loss: 1.177471\n",
      ">>> epoch: 374, batch index: 45, test loss: 1.609410, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 375, batch index: 385, train loss: 1.189527\n",
      ">>> epoch: 375, batch index: 45, test loss: 1.601418, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 376, batch index: 385, train loss: 1.217983\n",
      ">>> epoch: 376, batch index: 45, test loss: 1.616762, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 377, batch index: 385, train loss: 1.189525\n",
      ">>> epoch: 377, batch index: 45, test loss: 1.611890, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 378, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 378, batch index: 45, test loss: 1.612240, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 379, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 379, batch index: 45, test loss: 1.622261, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 380, batch index: 385, train loss: 1.213722\n",
      ">>> epoch: 380, batch index: 45, test loss: 1.605163, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 381, batch index: 385, train loss: 1.200847\n",
      ">>> epoch: 381, batch index: 45, test loss: 1.617733, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 382, batch index: 385, train loss: 1.212794\n",
      ">>> epoch: 382, batch index: 45, test loss: 1.606076, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 383, batch index: 385, train loss: 1.189105\n",
      ">>> epoch: 383, batch index: 45, test loss: 1.620047, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 384, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 384, batch index: 45, test loss: 1.597181, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 385, batch index: 385, train loss: 1.261806\n",
      ">>> epoch: 385, batch index: 45, test loss: 1.601205, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 386, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 386, batch index: 45, test loss: 1.603330, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 387, batch index: 385, train loss: 1.213028\n",
      ">>> epoch: 387, batch index: 45, test loss: 1.590631, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 388, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 388, batch index: 45, test loss: 1.603909, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 389, batch index: 385, train loss: 1.183388\n",
      ">>> epoch: 389, batch index: 45, test loss: 1.605146, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 390, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 390, batch index: 45, test loss: 1.598890, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 391, batch index: 385, train loss: 1.213623\n",
      ">>> epoch: 391, batch index: 45, test loss: 1.603437, acc: 0.549\n",
      "==================================================================\n",
      ">>> epoch: 392, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 392, batch index: 45, test loss: 1.615513, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 393, batch index: 385, train loss: 1.196529\n",
      ">>> epoch: 393, batch index: 45, test loss: 1.595540, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 394, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 394, batch index: 45, test loss: 1.603049, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 395, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 395, batch index: 45, test loss: 1.619593, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 396, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 396, batch index: 45, test loss: 1.609870, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 397, batch index: 385, train loss: 1.224920\n",
      ">>> epoch: 397, batch index: 45, test loss: 1.604584, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 398, batch index: 385, train loss: 1.189521\n",
      ">>> epoch: 398, batch index: 45, test loss: 1.606139, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 399, batch index: 385, train loss: 1.213616\n",
      ">>> epoch: 399, batch index: 45, test loss: 1.589030, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 400, batch index: 385, train loss: 1.259594\n",
      ">>> epoch: 400, batch index: 45, test loss: 1.615242, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 401, batch index: 385, train loss: 1.298045\n",
      ">>> epoch: 401, batch index: 45, test loss: 1.597493, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 402, batch index: 385, train loss: 1.189567\n",
      ">>> epoch: 402, batch index: 45, test loss: 1.604998, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 403, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 403, batch index: 45, test loss: 1.608506, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 404, batch index: 385, train loss: 1.213447\n",
      ">>> epoch: 404, batch index: 45, test loss: 1.630907, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 405, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 405, batch index: 45, test loss: 1.615208, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 406, batch index: 385, train loss: 1.237712\n",
      ">>> epoch: 406, batch index: 45, test loss: 1.594100, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 407, batch index: 385, train loss: 1.177513\n",
      ">>> epoch: 407, batch index: 45, test loss: 1.616242, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 408, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 408, batch index: 45, test loss: 1.618986, acc: 0.534\n",
      "==================================================================\n",
      ">>> epoch: 409, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 409, batch index: 45, test loss: 1.632012, acc: 0.529\n",
      "==================================================================\n",
      ">>> epoch: 410, batch index: 385, train loss: 1.212923\n",
      ">>> epoch: 410, batch index: 45, test loss: 1.616683, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 411, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 411, batch index: 45, test loss: 1.618289, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 412, batch index: 385, train loss: 1.213618\n",
      ">>> epoch: 412, batch index: 45, test loss: 1.604656, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 413, batch index: 385, train loss: 1.201818\n",
      ">>> epoch: 413, batch index: 45, test loss: 1.599662, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 414, batch index: 385, train loss: 1.211947\n",
      ">>> epoch: 414, batch index: 45, test loss: 1.604507, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 415, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 415, batch index: 45, test loss: 1.610430, acc: 0.530\n",
      "==================================================================\n",
      ">>> epoch: 416, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 416, batch index: 45, test loss: 1.602581, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 417, batch index: 385, train loss: 1.189543\n",
      ">>> epoch: 417, batch index: 45, test loss: 1.600274, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 418, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 418, batch index: 45, test loss: 1.611258, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 419, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 419, batch index: 45, test loss: 1.630781, acc: 0.532\n",
      "==================================================================\n",
      ">>> epoch: 420, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 420, batch index: 45, test loss: 1.607491, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 421, batch index: 385, train loss: 1.225111\n",
      ">>> epoch: 421, batch index: 45, test loss: 1.600790, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 422, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 422, batch index: 45, test loss: 1.599524, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 423, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 423, batch index: 45, test loss: 1.614555, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 424, batch index: 385, train loss: 1.213703\n",
      ">>> epoch: 424, batch index: 45, test loss: 1.612849, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 425, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 425, batch index: 45, test loss: 1.608533, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 426, batch index: 385, train loss: 1.165423\n",
      ">>> epoch: 426, batch index: 45, test loss: 1.603445, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 427, batch index: 385, train loss: 1.225729\n",
      ">>> epoch: 427, batch index: 45, test loss: 1.619846, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 428, batch index: 385, train loss: 1.225283\n",
      ">>> epoch: 428, batch index: 45, test loss: 1.622480, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 429, batch index: 385, train loss: 1.225558\n",
      ">>> epoch: 429, batch index: 45, test loss: 1.615950, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 430, batch index: 385, train loss: 1.225662\n",
      ">>> epoch: 430, batch index: 45, test loss: 1.616176, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 431, batch index: 385, train loss: 1.201570\n",
      ">>> epoch: 431, batch index: 45, test loss: 1.610050, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 432, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 432, batch index: 45, test loss: 1.596029, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 433, batch index: 385, train loss: 1.237807\n",
      ">>> epoch: 433, batch index: 45, test loss: 1.601458, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 434, batch index: 385, train loss: 1.189505\n",
      ">>> epoch: 434, batch index: 45, test loss: 1.613863, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 435, batch index: 385, train loss: 1.201858\n",
      ">>> epoch: 435, batch index: 45, test loss: 1.608267, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 436, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 436, batch index: 45, test loss: 1.595859, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 437, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 437, batch index: 45, test loss: 1.603077, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 438, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 438, batch index: 45, test loss: 1.614986, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 439, batch index: 385, train loss: 1.261807\n",
      ">>> epoch: 439, batch index: 45, test loss: 1.607930, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 440, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 440, batch index: 45, test loss: 1.604949, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 441, batch index: 385, train loss: 1.188955\n",
      ">>> epoch: 441, batch index: 45, test loss: 1.613611, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 442, batch index: 385, train loss: 1.176638\n",
      ">>> epoch: 442, batch index: 45, test loss: 1.618749, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 443, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 443, batch index: 45, test loss: 1.623589, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 444, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 444, batch index: 45, test loss: 1.612180, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 445, batch index: 385, train loss: 1.200869\n",
      ">>> epoch: 445, batch index: 45, test loss: 1.595232, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 446, batch index: 385, train loss: 1.177471\n",
      ">>> epoch: 446, batch index: 45, test loss: 1.611187, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 447, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 447, batch index: 45, test loss: 1.611776, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 448, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 448, batch index: 45, test loss: 1.609821, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 449, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 449, batch index: 45, test loss: 1.614104, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 450, batch index: 385, train loss: 1.177863\n",
      ">>> epoch: 450, batch index: 45, test loss: 1.598109, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 451, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 451, batch index: 45, test loss: 1.607000, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 452, batch index: 385, train loss: 1.189521\n",
      ">>> epoch: 452, batch index: 45, test loss: 1.596355, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 453, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 453, batch index: 45, test loss: 1.599877, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 454, batch index: 385, train loss: 1.237712\n",
      ">>> epoch: 454, batch index: 45, test loss: 1.602770, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 455, batch index: 385, train loss: 1.201558\n",
      ">>> epoch: 455, batch index: 45, test loss: 1.616149, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 456, batch index: 385, train loss: 1.237718\n",
      ">>> epoch: 456, batch index: 45, test loss: 1.613434, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 457, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 457, batch index: 45, test loss: 1.599885, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 458, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 458, batch index: 45, test loss: 1.598214, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 459, batch index: 385, train loss: 1.249758\n",
      ">>> epoch: 459, batch index: 45, test loss: 1.606363, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 460, batch index: 385, train loss: 1.203091\n",
      ">>> epoch: 460, batch index: 45, test loss: 1.604970, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 461, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 461, batch index: 45, test loss: 1.605342, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 462, batch index: 385, train loss: 1.189531\n",
      ">>> epoch: 462, batch index: 45, test loss: 1.609714, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 463, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 463, batch index: 45, test loss: 1.602835, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 464, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 464, batch index: 45, test loss: 1.602751, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 465, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 465, batch index: 45, test loss: 1.613643, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 466, batch index: 385, train loss: 1.188769\n",
      ">>> epoch: 466, batch index: 45, test loss: 1.604750, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 467, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 467, batch index: 45, test loss: 1.607258, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 468, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 468, batch index: 45, test loss: 1.611762, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 469, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 469, batch index: 45, test loss: 1.607325, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 470, batch index: 385, train loss: 1.189182\n",
      ">>> epoch: 470, batch index: 45, test loss: 1.610584, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 471, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 471, batch index: 45, test loss: 1.593671, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 472, batch index: 385, train loss: 1.200980\n",
      ">>> epoch: 472, batch index: 45, test loss: 1.594977, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 473, batch index: 385, train loss: 1.273855\n",
      ">>> epoch: 473, batch index: 45, test loss: 1.613894, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 474, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 474, batch index: 45, test loss: 1.616376, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 475, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 475, batch index: 45, test loss: 1.606554, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 476, batch index: 385, train loss: 1.223949\n",
      ">>> epoch: 476, batch index: 45, test loss: 1.601868, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 477, batch index: 385, train loss: 1.188984\n",
      ">>> epoch: 477, batch index: 45, test loss: 1.612888, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 478, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 478, batch index: 45, test loss: 1.607107, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 479, batch index: 385, train loss: 1.213345\n",
      ">>> epoch: 479, batch index: 45, test loss: 1.595366, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 480, batch index: 385, train loss: 1.201590\n",
      ">>> epoch: 480, batch index: 45, test loss: 1.610262, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 481, batch index: 385, train loss: 1.177496\n",
      ">>> epoch: 481, batch index: 45, test loss: 1.614862, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 482, batch index: 385, train loss: 1.249753\n",
      ">>> epoch: 482, batch index: 45, test loss: 1.609430, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 483, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 483, batch index: 45, test loss: 1.612899, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 484, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 484, batch index: 45, test loss: 1.616313, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 485, batch index: 385, train loss: 1.189526\n",
      ">>> epoch: 485, batch index: 45, test loss: 1.620178, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 486, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 486, batch index: 45, test loss: 1.605410, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 487, batch index: 385, train loss: 1.213098\n",
      ">>> epoch: 487, batch index: 45, test loss: 1.597453, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 488, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 488, batch index: 45, test loss: 1.611701, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 489, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 489, batch index: 45, test loss: 1.602576, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 490, batch index: 385, train loss: 1.201570\n",
      ">>> epoch: 490, batch index: 45, test loss: 1.608308, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 491, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 491, batch index: 45, test loss: 1.612471, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 492, batch index: 385, train loss: 1.273078\n",
      ">>> epoch: 492, batch index: 45, test loss: 1.619214, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 493, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 493, batch index: 45, test loss: 1.611508, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 494, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 494, batch index: 45, test loss: 1.619916, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 495, batch index: 385, train loss: 1.213627\n",
      ">>> epoch: 495, batch index: 45, test loss: 1.617975, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 496, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 496, batch index: 45, test loss: 1.609105, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 497, batch index: 385, train loss: 1.212938\n",
      ">>> epoch: 497, batch index: 45, test loss: 1.602993, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 498, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 498, batch index: 45, test loss: 1.594922, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 499, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 499, batch index: 45, test loss: 1.595193, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 500, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 500, batch index: 45, test loss: 1.601030, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 501, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 501, batch index: 45, test loss: 1.600546, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 502, batch index: 385, train loss: 1.165423\n",
      ">>> epoch: 502, batch index: 45, test loss: 1.603460, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 503, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 503, batch index: 45, test loss: 1.595483, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 504, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 504, batch index: 45, test loss: 1.616840, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 505, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 505, batch index: 45, test loss: 1.595798, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 506, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 506, batch index: 45, test loss: 1.608874, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 507, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 507, batch index: 45, test loss: 1.613410, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 508, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 508, batch index: 45, test loss: 1.619762, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 509, batch index: 385, train loss: 1.177474\n",
      ">>> epoch: 509, batch index: 45, test loss: 1.589797, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 510, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 510, batch index: 45, test loss: 1.604303, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 511, batch index: 385, train loss: 1.219583\n",
      ">>> epoch: 511, batch index: 45, test loss: 1.625726, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 512, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 512, batch index: 45, test loss: 1.607614, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 513, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 513, batch index: 45, test loss: 1.620571, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 514, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 514, batch index: 45, test loss: 1.619164, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 515, batch index: 385, train loss: 1.249395\n",
      ">>> epoch: 515, batch index: 45, test loss: 1.596916, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 516, batch index: 385, train loss: 1.225669\n",
      ">>> epoch: 516, batch index: 45, test loss: 1.606822, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 517, batch index: 385, train loss: 1.225728\n",
      ">>> epoch: 517, batch index: 45, test loss: 1.609443, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 518, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 518, batch index: 45, test loss: 1.609557, acc: 0.531\n",
      "==================================================================\n",
      ">>> epoch: 519, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 519, batch index: 45, test loss: 1.596148, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 520, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 520, batch index: 45, test loss: 1.617064, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 521, batch index: 385, train loss: 1.193333\n",
      ">>> epoch: 521, batch index: 45, test loss: 1.615327, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 522, batch index: 385, train loss: 1.204083\n",
      ">>> epoch: 522, batch index: 45, test loss: 1.612533, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 523, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 523, batch index: 45, test loss: 1.612934, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 524, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 524, batch index: 45, test loss: 1.591006, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 525, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 525, batch index: 45, test loss: 1.622695, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 526, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 526, batch index: 45, test loss: 1.610012, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 527, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 527, batch index: 45, test loss: 1.624778, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 528, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 528, batch index: 45, test loss: 1.611383, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 529, batch index: 385, train loss: 1.215260\n",
      ">>> epoch: 529, batch index: 45, test loss: 1.609548, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 530, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 530, batch index: 45, test loss: 1.603017, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 531, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 531, batch index: 45, test loss: 1.602279, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 532, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 532, batch index: 45, test loss: 1.597483, acc: 0.533\n",
      "==================================================================\n",
      ">>> epoch: 533, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 533, batch index: 45, test loss: 1.601749, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 534, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 534, batch index: 45, test loss: 1.596765, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 535, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 535, batch index: 45, test loss: 1.614693, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 536, batch index: 385, train loss: 1.213618\n",
      ">>> epoch: 536, batch index: 45, test loss: 1.604386, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 537, batch index: 385, train loss: 1.225667\n",
      ">>> epoch: 537, batch index: 45, test loss: 1.619598, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 538, batch index: 385, train loss: 1.201320\n",
      ">>> epoch: 538, batch index: 45, test loss: 1.617142, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 539, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 539, batch index: 45, test loss: 1.609987, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 540, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 540, batch index: 45, test loss: 1.611724, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 541, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 541, batch index: 45, test loss: 1.609036, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 542, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 542, batch index: 45, test loss: 1.623329, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 543, batch index: 385, train loss: 1.188686\n",
      ">>> epoch: 543, batch index: 45, test loss: 1.596330, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 544, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 544, batch index: 45, test loss: 1.609459, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 545, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 545, batch index: 45, test loss: 1.599784, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 546, batch index: 385, train loss: 1.261810\n",
      ">>> epoch: 546, batch index: 45, test loss: 1.618253, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 547, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 547, batch index: 45, test loss: 1.610145, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 548, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 548, batch index: 45, test loss: 1.610741, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 549, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 549, batch index: 45, test loss: 1.617748, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 550, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 550, batch index: 45, test loss: 1.593213, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 551, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 551, batch index: 45, test loss: 1.607057, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 552, batch index: 385, train loss: 1.177498\n",
      ">>> epoch: 552, batch index: 45, test loss: 1.616215, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 553, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 553, batch index: 45, test loss: 1.604427, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 554, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 554, batch index: 45, test loss: 1.628591, acc: 0.537\n",
      "==================================================================\n",
      ">>> epoch: 555, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 555, batch index: 45, test loss: 1.611373, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 556, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 556, batch index: 45, test loss: 1.610452, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 557, batch index: 385, train loss: 1.237716\n",
      ">>> epoch: 557, batch index: 45, test loss: 1.606595, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 558, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 558, batch index: 45, test loss: 1.611959, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 559, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 559, batch index: 45, test loss: 1.602801, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 560, batch index: 385, train loss: 1.225682\n",
      ">>> epoch: 560, batch index: 45, test loss: 1.615408, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 561, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 561, batch index: 45, test loss: 1.599224, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 562, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 562, batch index: 45, test loss: 1.614272, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 563, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 563, batch index: 45, test loss: 1.604447, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 564, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 564, batch index: 45, test loss: 1.595625, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 565, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 565, batch index: 45, test loss: 1.619601, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 566, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 566, batch index: 45, test loss: 1.608012, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 567, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 567, batch index: 45, test loss: 1.597799, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 568, batch index: 385, train loss: 1.225049\n",
      ">>> epoch: 568, batch index: 45, test loss: 1.599048, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 569, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 569, batch index: 45, test loss: 1.605142, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 570, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 570, batch index: 45, test loss: 1.609720, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 571, batch index: 385, train loss: 1.249759\n",
      ">>> epoch: 571, batch index: 45, test loss: 1.606802, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 572, batch index: 385, train loss: 1.177471\n",
      ">>> epoch: 572, batch index: 45, test loss: 1.610149, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 573, batch index: 385, train loss: 1.201494\n",
      ">>> epoch: 573, batch index: 45, test loss: 1.611839, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 574, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 574, batch index: 45, test loss: 1.592930, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 575, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 575, batch index: 45, test loss: 1.613267, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 576, batch index: 385, train loss: 1.249116\n",
      ">>> epoch: 576, batch index: 45, test loss: 1.619820, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 577, batch index: 385, train loss: 1.182871\n",
      ">>> epoch: 577, batch index: 45, test loss: 1.613385, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 578, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 578, batch index: 45, test loss: 1.604711, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 579, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 579, batch index: 45, test loss: 1.606087, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 580, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 580, batch index: 45, test loss: 1.600705, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 581, batch index: 385, train loss: 1.213340\n",
      ">>> epoch: 581, batch index: 45, test loss: 1.601325, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 582, batch index: 385, train loss: 1.225632\n",
      ">>> epoch: 582, batch index: 45, test loss: 1.616732, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 583, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 583, batch index: 45, test loss: 1.608372, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 584, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 584, batch index: 45, test loss: 1.606246, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 585, batch index: 385, train loss: 1.189568\n",
      ">>> epoch: 585, batch index: 45, test loss: 1.593069, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 586, batch index: 385, train loss: 1.177535\n",
      ">>> epoch: 586, batch index: 45, test loss: 1.604160, acc: 0.539\n",
      "==================================================================\n",
      ">>> epoch: 587, batch index: 385, train loss: 1.189555\n",
      ">>> epoch: 587, batch index: 45, test loss: 1.581380, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 588, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 588, batch index: 45, test loss: 1.604423, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 589, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 589, batch index: 45, test loss: 1.597235, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 590, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 590, batch index: 45, test loss: 1.600262, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 591, batch index: 385, train loss: 1.177474\n",
      ">>> epoch: 591, batch index: 45, test loss: 1.604712, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 592, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 592, batch index: 45, test loss: 1.598784, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 593, batch index: 385, train loss: 1.201571\n",
      ">>> epoch: 593, batch index: 45, test loss: 1.594951, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 594, batch index: 385, train loss: 1.201572\n",
      ">>> epoch: 594, batch index: 45, test loss: 1.607621, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 595, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 595, batch index: 45, test loss: 1.600428, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 596, batch index: 385, train loss: 1.189520\n",
      ">>> epoch: 596, batch index: 45, test loss: 1.593952, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 597, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 597, batch index: 45, test loss: 1.588930, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 598, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 598, batch index: 45, test loss: 1.622649, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 599, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 599, batch index: 45, test loss: 1.593806, acc: 0.552\n",
      "==================================================================\n",
      ">>> epoch: 600, batch index: 385, train loss: 1.225570\n",
      ">>> epoch: 600, batch index: 45, test loss: 1.604029, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 601, batch index: 385, train loss: 1.212911\n",
      ">>> epoch: 601, batch index: 45, test loss: 1.605879, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 602, batch index: 385, train loss: 1.189522\n",
      ">>> epoch: 602, batch index: 45, test loss: 1.599494, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 603, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 603, batch index: 45, test loss: 1.599735, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 604, batch index: 385, train loss: 1.189548\n",
      ">>> epoch: 604, batch index: 45, test loss: 1.613188, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 605, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 605, batch index: 45, test loss: 1.601951, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 606, batch index: 385, train loss: 1.190044\n",
      ">>> epoch: 606, batch index: 45, test loss: 1.605644, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 607, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 607, batch index: 45, test loss: 1.616709, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 608, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 608, batch index: 45, test loss: 1.603803, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 609, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 609, batch index: 45, test loss: 1.613078, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 610, batch index: 385, train loss: 1.189564\n",
      ">>> epoch: 610, batch index: 45, test loss: 1.599765, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 611, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 611, batch index: 45, test loss: 1.605825, acc: 0.538\n",
      "==================================================================\n",
      ">>> epoch: 612, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 612, batch index: 45, test loss: 1.605605, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 613, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 613, batch index: 45, test loss: 1.588188, acc: 0.550\n",
      "==================================================================\n",
      ">>> epoch: 614, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 614, batch index: 45, test loss: 1.608269, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 615, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 615, batch index: 45, test loss: 1.612300, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 616, batch index: 385, train loss: 1.165423\n",
      ">>> epoch: 616, batch index: 45, test loss: 1.607783, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 617, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 617, batch index: 45, test loss: 1.602448, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 618, batch index: 385, train loss: 1.201568\n",
      ">>> epoch: 618, batch index: 45, test loss: 1.617967, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 619, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 619, batch index: 45, test loss: 1.610977, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 620, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 620, batch index: 45, test loss: 1.616486, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 621, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 621, batch index: 45, test loss: 1.614288, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 622, batch index: 385, train loss: 1.213614\n",
      ">>> epoch: 622, batch index: 45, test loss: 1.620940, acc: 0.535\n",
      "==================================================================\n",
      ">>> epoch: 623, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 623, batch index: 45, test loss: 1.603770, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 624, batch index: 385, train loss: 1.213649\n",
      ">>> epoch: 624, batch index: 45, test loss: 1.609318, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 625, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 625, batch index: 45, test loss: 1.613486, acc: 0.536\n",
      "==================================================================\n",
      ">>> epoch: 626, batch index: 385, train loss: 1.213615\n",
      ">>> epoch: 626, batch index: 45, test loss: 1.600262, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 627, batch index: 385, train loss: 1.213618\n",
      ">>> epoch: 627, batch index: 45, test loss: 1.603888, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 628, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 628, batch index: 45, test loss: 1.600310, acc: 0.541\n",
      "==================================================================\n",
      ">>> epoch: 629, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 629, batch index: 45, test loss: 1.600913, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 630, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 630, batch index: 45, test loss: 1.595618, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 631, batch index: 385, train loss: 1.201567\n",
      ">>> epoch: 631, batch index: 45, test loss: 1.612136, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 632, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 632, batch index: 45, test loss: 1.581771, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 633, batch index: 385, train loss: 1.177470\n",
      ">>> epoch: 633, batch index: 45, test loss: 1.619110, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 634, batch index: 385, train loss: 1.165439\n",
      ">>> epoch: 634, batch index: 45, test loss: 1.612403, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 635, batch index: 385, train loss: 1.189517\n",
      ">>> epoch: 635, batch index: 45, test loss: 1.597648, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 636, batch index: 385, train loss: 1.200985\n",
      ">>> epoch: 636, batch index: 45, test loss: 1.607183, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 637, batch index: 385, train loss: 1.237711\n",
      ">>> epoch: 637, batch index: 45, test loss: 1.613142, acc: 0.546\n",
      "==================================================================\n",
      ">>> epoch: 638, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 638, batch index: 45, test loss: 1.607029, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 639, batch index: 385, train loss: 1.189519\n",
      ">>> epoch: 639, batch index: 45, test loss: 1.608916, acc: 0.540\n",
      "==================================================================\n",
      ">>> epoch: 640, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 640, batch index: 45, test loss: 1.595698, acc: 0.550\n",
      "==================================================================\n",
      ">>> epoch: 641, batch index: 385, train loss: 1.201566\n",
      ">>> epoch: 641, batch index: 45, test loss: 1.618618, acc: 0.548\n",
      "==================================================================\n",
      ">>> epoch: 642, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 642, batch index: 45, test loss: 1.606369, acc: 0.542\n",
      "==================================================================\n",
      ">>> epoch: 643, batch index: 385, train loss: 1.225663\n",
      ">>> epoch: 643, batch index: 45, test loss: 1.596374, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 644, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 644, batch index: 45, test loss: 1.601033, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 645, batch index: 385, train loss: 1.213616\n",
      ">>> epoch: 645, batch index: 45, test loss: 1.596452, acc: 0.547\n",
      "==================================================================\n",
      ">>> epoch: 646, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 646, batch index: 45, test loss: 1.597338, acc: 0.543\n",
      "==================================================================\n",
      ">>> epoch: 647, batch index: 385, train loss: 1.209624\n",
      ">>> epoch: 647, batch index: 45, test loss: 1.610550, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 648, batch index: 385, train loss: 1.165422\n",
      ">>> epoch: 648, batch index: 45, test loss: 1.615580, acc: 0.545\n",
      "==================================================================\n",
      ">>> epoch: 649, batch index: 385, train loss: 1.189518\n",
      ">>> epoch: 649, batch index: 45, test loss: 1.618782, acc: 0.544\n",
      "==================================================================\n",
      ">>> epoch: 650, batch index: 385, train loss: 1.203200\n",
      ">>> epoch: 650, batch index: 45, test loss: 1.614652, acc: 0.542\n",
      "==================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a9a14b0e11ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mave_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mave_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.9\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#back propagation with calculated loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculate gradient and step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m600\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global epoch #declear epoch global, to be used later by torch.save() \n",
    "\n",
    "for epoch in range(1000):\n",
    "\n",
    "    ave_loss = 0\n",
    "    global loss #declear loss global, to be used later by torch.save() \n",
    "    for batch_idx, diction in enumerate(train_loader):\n",
    "        model.train() #set model to traning mode\n",
    "        optimizer.zero_grad()\n",
    "        x, target = diction['image'], diction['category'] #extract training data for this batch\n",
    "        x = x.unsqueeze(1)\n",
    "        x, target = x.float(), target.float() #set datatype\n",
    "        x, target = x.to(device), target.to(device) #transfer to GPU\n",
    "        x, target = Variable(x), Variable(target) #set to pytorch datatype: variable\n",
    "\n",
    "        out = model(x) #forward pass\n",
    "\n",
    "        loss = criterion(out, target.long()) #calculate loss\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1 \n",
    "        loss.backward() #back propagation with calculated loss\n",
    "        optimizer.step() #calculate gradient and step\n",
    "        \n",
    "        if (batch_idx + 1) % 600 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print('>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx + 1, loss))\n",
    "        training_stats['tarining_loss'].append(loss)\n",
    "            \n",
    "        \n",
    "\n",
    "    correct, ave_loss, total_cnt = 0, 0, 0\n",
    "    for batch_idx, diction in enumerate(val_loader):\n",
    "        model.eval() #set model to evaluation mode\n",
    "        x, target = diction['image'], diction['category']\n",
    "        x = x.unsqueeze(1)\n",
    "        x, target = x.float(), target.float()\n",
    "        x, target = x.to(device), target.to(device)\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target.long()) #calculate loss\n",
    "\n",
    "        pred_label = out.data\n",
    "        pred_label = pred_label.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        target = target.long()\n",
    "        \n",
    "        total_cnt += x.data.size()[0]\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1 #smooth average\n",
    "        correct += pred_label.eq(target.view_as(pred_label)).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 600 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "            print(\n",
    "            '>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct * 1.0 / total_cnt))\n",
    "            print('==================================================================') \n",
    "            \n",
    "        training_stats['validation_loss'].append(ave_loss)\n",
    "        training_stats['accuracy'].append(correct * 1.0 / total_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation                   OPS         \n",
      "--------------------------  ----------  \n",
      "/onnx::Conv                 46080000    \n",
      "/onnx::BatchNormalization   1843200     \n",
      "/onnx::Relu                 1843200     \n",
      "/onnx::MaxPool              1843200     \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Add                  230400      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Add                  230400      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   460800      \n",
      "/onnx::Add                  230400      \n",
      "/onnx::Relu                 460800      \n",
      "/onnx::Conv                 66355200    \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Conv                 7372800     \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Add                  115200      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Add                  115200      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Add                  115200      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   230400      \n",
      "/onnx::Add                  115200      \n",
      "/onnx::Relu                 230400      \n",
      "/onnx::Conv                 66355200    \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Conv                 7372800     \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 132710400   \n",
      "/onnx::BatchNormalization   115200      \n",
      "/onnx::Add                  57600       \n",
      "/onnx::Relu                 115200      \n",
      "/onnx::Conv                 117964800   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::Conv                 235929600   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Conv                 13107200    \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Add                  51200       \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::Conv                 235929600   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::Conv                 235929600   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Add                  51200       \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::Conv                 235929600   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::Conv                 235929600   \n",
      "/onnx::BatchNormalization   102400      \n",
      "/onnx::Add                  51200       \n",
      "/onnx::Relu                 102400      \n",
      "/onnx::GlobalAveragePool    51200       \n",
      "/onnx::Gemm                 6553600     \n",
      "/onnx::Relu                 25600       \n",
      "/onnx::Gemm                 3276800     \n",
      "/onnx::Relu                 12800       \n",
      "/onnx::Gemm                 44800       \n",
      "-------------------------   ---------   \n",
      "Input size: (25, 1, 48, 48)\n",
      "4,720,108,800 FLOPs or approx. 4.72 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4720108800,\n",
       " [['/onnx::Conv', 46080000],\n",
       "  ['/onnx::BatchNormalization', 1843200],\n",
       "  ['/onnx::Relu', 1843200],\n",
       "  ['/onnx::MaxPool', 1843200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Add', 230400],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Add', 230400],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 460800],\n",
       "  ['/onnx::Add', 230400],\n",
       "  ['/onnx::Relu', 460800],\n",
       "  ['/onnx::Conv', 66355200],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Conv', 7372800],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Add', 115200],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Add', 115200],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Add', 115200],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 230400],\n",
       "  ['/onnx::Add', 115200],\n",
       "  ['/onnx::Relu', 230400],\n",
       "  ['/onnx::Conv', 66355200],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Conv', 7372800],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 132710400],\n",
       "  ['/onnx::BatchNormalization', 115200],\n",
       "  ['/onnx::Add', 57600],\n",
       "  ['/onnx::Relu', 115200],\n",
       "  ['/onnx::Conv', 117964800],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::Conv', 235929600],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Conv', 13107200],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Add', 51200],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::Conv', 235929600],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::Conv', 235929600],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Add', 51200],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::Conv', 235929600],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::Conv', 235929600],\n",
       "  ['/onnx::BatchNormalization', 102400],\n",
       "  ['/onnx::Add', 51200],\n",
       "  ['/onnx::Relu', 102400],\n",
       "  ['/onnx::GlobalAveragePool', 51200],\n",
       "  ['/onnx::Gemm', 6553600],\n",
       "  ['/onnx::Relu', 25600],\n",
       "  ['/onnx::Gemm', 3276800],\n",
       "  ['/onnx::Relu', 12800],\n",
       "  ['/onnx::Gemm', 44800]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pthflops import count_ops\n",
    "\n",
    "inp = torch.Tensor(np.zeros(57600).reshape(25, 1, 48, 48))\n",
    "\n",
    "inp = inp.cuda()\n",
    "inp = inp.to(device)\n",
    "\n",
    "count_ops(model, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "'''\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['tarining_loss'])), training_stats['tarining_loss'])\n",
    "plt.ylabel('training_loss')\n",
    "plt.title('iter')\n",
    "\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['validation_loss'])), training_stats['validation_loss'])\n",
    "plt.ylabel('validation_loss')\n",
    "plt.title('iter')\n",
    "'''\n",
    "plt.subplot()\n",
    "plt.plot(np.arange(len(training_stats['accuracy'])), training_stats['accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('iter')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(training_stats['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
