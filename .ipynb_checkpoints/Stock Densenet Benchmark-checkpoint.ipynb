{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "image_dir = r\"C:\\Users\\Leo's PC\\Desktop\\images\"\n",
    "csv_dir = r\"C:\\Users\\Leo's PC\\Desktop\\AVA.txt\"\n",
    "checkpoint_save_dir = r\"C:\\Users\\Leo's PC\\PycharmProjects\\PD\\Model Checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVA Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AVADataset(Dataset):\n",
    "    \"\"\"AVA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, file_dir, start, end, class_size=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            file_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            start&end: specify the range in the raw data to construct the dataset from. For spliting calidation and test sets.\n",
    "            class_size = make the amount of images in each rating group equal to class_size.\n",
    "        \"\"\"\n",
    "        self.csv = pd.read_csv(csv_file, sep=' ')\n",
    "        self.file_dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.diction = {}\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.class_size = class_size\n",
    "\n",
    "        for _, _, files in os.walk(self.file_dir): #get a list of file names in the dataset folder\n",
    "            self.img_name_array = files\n",
    "\n",
    "        self.img_name_array = self.img_name_array[start:end] #limit the range of file names to start:end\n",
    "\n",
    "        index = 0\n",
    "        self.rat_distribution = np.zeros(9, dtype=np.int64)\n",
    "        print(\"index =\", index, \"AVA Dataset initialization begin...\")\n",
    "        print(\"Rating distribution initialized: \", self.rat_distribution)\n",
    "        for csv_idx, row in self.csv.iterrows(): #traverse the enitre csv file\n",
    "            image_name = row[1]\n",
    "            if (str(image_name) + '.jpg') in self.img_name_array: \n",
    "                #only add [image_name, avg_rat] to diction when it's in the img_name_array\n",
    "               \n",
    "                rating_array = np.array(row[2:12])\n",
    "                avg_rat = np.argmax(rating_array) + 1\n",
    "                if class_size: #when it's needed to limit class amount\n",
    "                    if self.rat_distribution[avg_rat - 2] < class_size: #only enter if under limit\n",
    "                        self.diction[index] = [image_name, avg_rat] #append\n",
    "                        self.rat_distribution[avg_rat - 2] += 1 #update rat_distribution\n",
    "                        index += 1 #update index\n",
    "                else: #when class amount is not specified. Just append.\n",
    "                    self.diction[index] = [image_name, avg_rat] #append\n",
    "                    self.rat_distribution[avg_rat - 2] += 1 #update rat_distribution\n",
    "                    index += 1 #update index\n",
    "                if csv_idx % 10000 == 0:\n",
    "                    print('csv_idx:', csv_idx, 'index:', index, 'img_name', image_name, 'avg_rat', avg_rat,\"\\n\",\n",
    "                          \" - Current rating distribution is: \", self.rat_distribution)\n",
    "            \n",
    "        print(\"Stage 1 loading complete. Current rating distribution is: \", self.rat_distribution)\n",
    "\n",
    "        plt.bar(np.arange(9), self.rat_distribution, 0.35) #(indeces, data, width)\n",
    "        plt.ylabel('Number of Pictures')\n",
    "        plt.title('Current Rating Distribution')\n",
    "        plt.xticks(np.arange(9), ('1', '2', '3', '4', '5', '6', '7', '8', '9'))\n",
    "        plt.show()\n",
    "        \n",
    "        #now that all classes are <= class_limit, we need to make all of them = class limit\n",
    "        if class_size: #only proceed when it's needed to limit class amount\n",
    "            self.distribution_multiplier = np.ceil(np.divide(np.full(9, self.class_size), self.rat_distribution))\n",
    "            #np.ceil rounds up to provide enough images in each class. np.fill creates an array with 9 elements = class_size\n",
    "            print(\"Stage 2 begin. Target class size is:\", class_size, \"Distribution multiplier is:\", self.distribution_multiplier)\n",
    "            self.additional_diction = {}\n",
    "            for item in self.diction.items(): #traverse the self.diction\n",
    "                img_name = item[1][0]\n",
    "                avg_rat = item[1][1]\n",
    "                if self.rat_distribution[avg_rat - 2] < class_size: #only enter if < class_limit\n",
    "                    for i in range(int(self.distribution_multiplier[avg_rat - 2])):\n",
    "                        self.additional_diction[len(self.diction) + len(self.additional_diction)] = [image_name, avg_rat]\n",
    "                        #append the same image at the end of additional_diction (distribution_multiplier) times\n",
    "                    self.rat_distribution[avg_rat - 2] += self.distribution_multiplier[avg_rat - 2] #update rat_distribution\n",
    "            self.diction.update(self.additional_diction) #combine diction and additional_diction\n",
    "                    \n",
    "        print(\"AVA Dataset initialization complete. Rating distribution is: \", self.rat_distribution, \"\\n\",\n",
    "        \"contains\", len(self.diction), \"items.\")\n",
    "        \n",
    "        plt.bar(np.arange(9), self.rat_distribution, 0.35) #(indeces, data, width)\n",
    "        plt.ylabel('Number of Pictures')\n",
    "        plt.title('Current Rating Distribution')\n",
    "        plt.xticks(np.arange(9), ('1', '2', '3', '4', '5', '6', '7', '8', '9'))\n",
    "        plt.show()\n",
    "                        \n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.diction)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.diction[idx][0]\n",
    "        rat_avg = self.diction[idx][1]\n",
    "        directory = self.file_dir + \"\\\\\" + str(img_name) + '.jpg'\n",
    "        image = cv2.imread(directory, cv2.IMREAD_COLOR)\n",
    "        sample = {'image': np.array(image, dtype=float), 'rating': np.array(rat_avg, dtype=float)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Dataset Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "     output_size (tuple or int): Desired output size. If tuple, output is\n",
    "         matched to output_size. If int, smaller of image edges is matched\n",
    "         to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = cv2.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'rating': rating}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        rating = np.array(rating)\n",
    "        image = image.transpose((2, 0, 1)) #swap color axis because: numpy image: H x W x C & torch image: C X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                 'rating': torch.from_numpy(rating)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset Instances and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AVA_Train = AVADataset(csv_file=csv_dir, file_dir=image_dir, start=0, end=209999, transform=transforms.Compose([Rescale((224, 224)), ToTensor()]))\n",
    "AVA_Validation = AVADataset(csv_file=csv_dir, file_dir=image_dir, start=210000, end=229999, transform=transforms.Compose([Rescale((224, 224)), ToTensor()]))\n",
    "# AVA_Test = AVADataset(csv_file=csv_dir, file_dir=image_dir, start=230000, end=249999, transform=transforms.Compose([Rescale((224, 224)), ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save dataset objects as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./AVA_dataset_pkl/train_imbalanced.pkl', 'wb') as fp:\n",
    "        pickle.dump(AVA_Train, fp)\n",
    "with open('./AVA_dataset_pkl/validation_imbalanced.pkl', 'wb') as fp:\n",
    "        pickle.dump(AVA_Validation, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load dataset pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./AVA_dataset_pkl/train_imbalanced.pkl', 'rb') as fp:\n",
    "    AVA_Train = pickle.load(fp)\n",
    "with open('./AVA_dataset_pkl/validation_imbalanced.pkl', 'rb') as fp:\n",
    "    AVA_Validation = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=AVA_Train, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(dataset=AVA_Validation, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureNet, self).__init__()\n",
    "        \n",
    "        self.densenet = torchvision.models.densenet121(pretrained=True, progress=True).features\n",
    "\n",
    "        self.dense_input = nn.Sequential(self.densenet.conv0, self.densenet.norm0, self.densenet.relu0, self.densenet.pool0)\n",
    "        self.dense_block1 = self.densenet.denseblock1\n",
    "        self.dense_block2 = self.densenet.denseblock2\n",
    "        self.dense_block3 = self.densenet.denseblock3\n",
    "        self.dense_block4 = self.densenet.denseblock4\n",
    "        \n",
    "        self.dense_transition1 = self.densenet.transition1 \n",
    "        self.dense_transition2 = self.densenet.transition2 \n",
    "        self.dense_transition3 = self.densenet.transition3 \n",
    "\n",
    "        self.pooling = nn.AvgPool2d(kernel_size = [7, 7], stride=7, padding=0)\n",
    "        self.classifier = nn.Linear(1024, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        x = self.dense_input(x)\n",
    "        x = self.dense_block1(x)\n",
    "        x = self.dense_transition1(x)\n",
    "        x = self.dense_block2(x)\n",
    "        x = self.dense_transition2(x)\n",
    "        x = self.dense_block3(x)\n",
    "        x = self.dense_transition3(x)\n",
    "        x = self.dense_block4(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"FeatureNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FeatureNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.dense_input.parameters():\n",
    "    p.requires_grad == False\n",
    "\n",
    "for p in model.dense_block1.parameters():\n",
    "    p.requires_grad == False\n",
    "        \n",
    "for p in model.dense_block2.parameters():\n",
    "    p.requires_grad == True\n",
    "        \n",
    "for p in model.dense_transition1.parameters():\n",
    "    p.requires_grad == False\n",
    "        \n",
    "for p in model.dense_transition2.parameters():\n",
    "    p.requires_grad == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for single-GPU/CPU training ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for Data Parallel training ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.cuda()\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic mixed-percision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = {}\n",
    "training_stats['tarining_loss'] = []\n",
    "training_stats['validation_loss'] = []\n",
    "training_stats['accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\cuda\\nccl.py:14: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 0.002796\n",
      "==>>> epoch: 0, batch index: 200, train loss: 0.002651\n",
      "==>>> epoch: 0, batch index: 300, train loss: 0.004767\n",
      "==>>> epoch: 0, batch index: 400, train loss: 0.003166\n",
      "==>>> epoch: 0, batch index: 500, train loss: 0.003713\n",
      "==>>> epoch: 0, batch index: 600, train loss: 0.003476\n",
      "==>>> epoch: 0, batch index: 700, train loss: 0.002591\n",
      "==>>> epoch: 0, batch index: 800, train loss: 0.002422\n",
      "==>>> epoch: 0, batch index: 900, train loss: 0.003998\n",
      "==>>> epoch: 0, batch index: 1000, train loss: 0.003593\n",
      "==>>> epoch: 0, batch index: 1100, train loss: 0.001894\n",
      "==>>> epoch: 0, batch index: 1200, train loss: 0.001655\n",
      "==>>> epoch: 0, batch index: 1300, train loss: 0.002851\n",
      "==>>> epoch: 0, batch index: 1400, train loss: 0.002113\n",
      "==>>> epoch: 0, batch index: 1500, train loss: 0.004777\n",
      "==>>> epoch: 0, batch index: 1600, train loss: 0.002496\n",
      "==>>> epoch: 0, batch index: 1700, train loss: 0.002353\n",
      "==>>> epoch: 0, batch index: 1800, train loss: 0.002224\n",
      "==>>> epoch: 0, batch index: 1900, train loss: 0.002363\n",
      "==>>> epoch: 0, batch index: 2000, train loss: 0.004111\n",
      "==>>> epoch: 0, batch index: 2100, train loss: 0.002534\n",
      "==>>> epoch: 0, batch index: 2200, train loss: 0.003391\n",
      "==>>> epoch: 0, batch index: 2300, train loss: 0.002620\n",
      "==>>> epoch: 0, batch index: 2400, train loss: 0.002565\n",
      "==>>> epoch: 0, batch index: 2500, train loss: 0.002010\n",
      "==>>> epoch: 0, batch index: 2600, train loss: 0.002491\n",
      "==>>> epoch: 0, batch index: 2700, train loss: 0.003098\n",
      "==>>> epoch: 0, batch index: 2800, train loss: 0.004035\n",
      "==>>> epoch: 0, batch index: 2900, train loss: 0.002787\n",
      "==>>> epoch: 0, batch index: 3000, train loss: 0.002352\n",
      "==>>> epoch: 0, batch index: 3100, train loss: 0.004982\n",
      "==>>> epoch: 0, batch index: 3200, train loss: 0.004187\n",
      "==>>> epoch: 0, batch index: 3282, train loss: 0.001691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, test loss: 0.000658, acc: 0.585\n",
      "==>>> epoch: 0, batch index: 200, test loss: 0.001283, acc: 0.587\n",
      "==>>> epoch: 0, batch index: 300, test loss: 0.001837, acc: 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 313, test loss: 0.001905, acc: 0.585\n",
      "==>>> epoch: 1, batch index: 100, train loss: 0.002629\n",
      "==>>> epoch: 1, batch index: 200, train loss: 0.002352\n",
      "==>>> epoch: 1, batch index: 300, train loss: 0.002486\n",
      "==>>> epoch: 1, batch index: 400, train loss: 0.002615\n",
      "==>>> epoch: 1, batch index: 500, train loss: 0.002286\n",
      "==>>> epoch: 1, batch index: 600, train loss: 0.002692\n",
      "==>>> epoch: 1, batch index: 700, train loss: 0.001946\n",
      "==>>> epoch: 1, batch index: 800, train loss: 0.001491\n",
      "==>>> epoch: 1, batch index: 900, train loss: 0.002903\n",
      "==>>> epoch: 1, batch index: 1000, train loss: 0.001812\n",
      "==>>> epoch: 1, batch index: 1100, train loss: 0.002852\n",
      "==>>> epoch: 1, batch index: 1200, train loss: 0.003094\n",
      "==>>> epoch: 1, batch index: 1300, train loss: 0.003442\n",
      "==>>> epoch: 1, batch index: 1400, train loss: 0.002244\n",
      "==>>> epoch: 1, batch index: 1500, train loss: 0.002824\n",
      "==>>> epoch: 1, batch index: 1600, train loss: 0.002073\n",
      "==>>> epoch: 1, batch index: 1700, train loss: 0.002571\n",
      "==>>> epoch: 1, batch index: 1800, train loss: 0.002208\n",
      "==>>> epoch: 1, batch index: 1900, train loss: 0.002847\n",
      "==>>> epoch: 1, batch index: 2000, train loss: 0.003851\n",
      "==>>> epoch: 1, batch index: 2100, train loss: 0.002169\n",
      "==>>> epoch: 1, batch index: 2200, train loss: 0.002311\n",
      "==>>> epoch: 1, batch index: 2300, train loss: 0.001930\n",
      "==>>> epoch: 1, batch index: 2400, train loss: 0.002034\n",
      "==>>> epoch: 1, batch index: 2500, train loss: 0.005178\n",
      "==>>> epoch: 1, batch index: 2600, train loss: 0.002580\n",
      "==>>> epoch: 1, batch index: 2700, train loss: 0.001934\n",
      "==>>> epoch: 1, batch index: 2800, train loss: 0.002937\n",
      "==>>> epoch: 1, batch index: 2900, train loss: 0.005028\n",
      "==>>> epoch: 1, batch index: 3000, train loss: 0.002133\n",
      "==>>> epoch: 1, batch index: 3100, train loss: 0.002627\n",
      "==>>> epoch: 1, batch index: 3200, train loss: 0.002759\n",
      "==>>> epoch: 1, batch index: 3282, train loss: 0.003848\n",
      "==>>> epoch: 1, batch index: 100, test loss: 0.000649, acc: 0.587\n",
      "==>>> epoch: 1, batch index: 200, test loss: 0.001286, acc: 0.579\n",
      "==>>> epoch: 1, batch index: 300, test loss: 0.001832, acc: 0.583\n",
      "==>>> epoch: 1, batch index: 313, test loss: 0.001913, acc: 0.582\n",
      "==>>> epoch: 2, batch index: 100, train loss: 0.002562\n",
      "==>>> epoch: 2, batch index: 200, train loss: 0.001599\n",
      "==>>> epoch: 2, batch index: 300, train loss: 0.002434\n",
      "==>>> epoch: 2, batch index: 400, train loss: 0.003655\n",
      "==>>> epoch: 2, batch index: 500, train loss: 0.002658\n",
      "==>>> epoch: 2, batch index: 600, train loss: 0.003255\n",
      "==>>> epoch: 2, batch index: 700, train loss: 0.002001\n",
      "==>>> epoch: 2, batch index: 800, train loss: 0.002229\n",
      "==>>> epoch: 2, batch index: 900, train loss: 0.004457\n",
      "==>>> epoch: 2, batch index: 1000, train loss: 0.001635\n",
      "==>>> epoch: 2, batch index: 1100, train loss: 0.001964\n",
      "==>>> epoch: 2, batch index: 1200, train loss: 0.002766\n",
      "==>>> epoch: 2, batch index: 1300, train loss: 0.002479\n",
      "==>>> epoch: 2, batch index: 1400, train loss: 0.001653\n",
      "==>>> epoch: 2, batch index: 1500, train loss: 0.003647\n",
      "==>>> epoch: 2, batch index: 1600, train loss: 0.003302\n",
      "==>>> epoch: 2, batch index: 1700, train loss: 0.002126\n",
      "==>>> epoch: 2, batch index: 1800, train loss: 0.003366\n",
      "==>>> epoch: 2, batch index: 1900, train loss: 0.003491\n",
      "==>>> epoch: 2, batch index: 2000, train loss: 0.002644\n",
      "==>>> epoch: 2, batch index: 2100, train loss: 0.002077\n",
      "==>>> epoch: 2, batch index: 2200, train loss: 0.003067\n",
      "==>>> epoch: 2, batch index: 2300, train loss: 0.002116\n",
      "==>>> epoch: 2, batch index: 2400, train loss: 0.003152\n",
      "==>>> epoch: 2, batch index: 2500, train loss: 0.002302\n",
      "==>>> epoch: 2, batch index: 2600, train loss: 0.001939\n",
      "==>>> epoch: 2, batch index: 2700, train loss: 0.002627\n",
      "==>>> epoch: 2, batch index: 2800, train loss: 0.002775\n",
      "==>>> epoch: 2, batch index: 2900, train loss: 0.002138\n",
      "==>>> epoch: 2, batch index: 3000, train loss: 0.002740\n",
      "==>>> epoch: 2, batch index: 3100, train loss: 0.003088\n",
      "==>>> epoch: 2, batch index: 3200, train loss: 0.003009\n",
      "==>>> epoch: 2, batch index: 3282, train loss: 0.001945\n",
      "==>>> epoch: 2, batch index: 100, test loss: 0.000702, acc: 0.580\n",
      "==>>> epoch: 2, batch index: 200, test loss: 0.001327, acc: 0.576\n",
      "==>>> epoch: 2, batch index: 300, test loss: 0.001879, acc: 0.577\n",
      "==>>> epoch: 2, batch index: 313, test loss: 0.001949, acc: 0.577\n",
      "==>>> epoch: 3, batch index: 100, train loss: 0.002781\n",
      "==>>> epoch: 3, batch index: 200, train loss: 0.003179\n",
      "==>>> epoch: 3, batch index: 300, train loss: 0.001434\n",
      "==>>> epoch: 3, batch index: 400, train loss: 0.003270\n",
      "==>>> epoch: 3, batch index: 500, train loss: 0.002028\n",
      "==>>> epoch: 3, batch index: 600, train loss: 0.002853\n",
      "==>>> epoch: 3, batch index: 700, train loss: 0.001855\n",
      "==>>> epoch: 3, batch index: 800, train loss: 0.002663\n",
      "==>>> epoch: 3, batch index: 900, train loss: 0.002130\n",
      "==>>> epoch: 3, batch index: 1000, train loss: 0.002441\n",
      "==>>> epoch: 3, batch index: 1100, train loss: 0.002697\n",
      "==>>> epoch: 3, batch index: 1200, train loss: 0.002839\n",
      "==>>> epoch: 3, batch index: 1300, train loss: 0.002330\n",
      "==>>> epoch: 3, batch index: 1400, train loss: 0.002923\n",
      "==>>> epoch: 3, batch index: 1500, train loss: 0.002245\n",
      "==>>> epoch: 3, batch index: 1600, train loss: 0.002056\n",
      "==>>> epoch: 3, batch index: 1700, train loss: 0.002485\n",
      "==>>> epoch: 3, batch index: 1800, train loss: 0.002269\n",
      "==>>> epoch: 3, batch index: 1900, train loss: 0.003215\n",
      "==>>> epoch: 3, batch index: 2000, train loss: 0.003444\n",
      "==>>> epoch: 3, batch index: 2100, train loss: 0.002360\n",
      "==>>> epoch: 3, batch index: 2200, train loss: 0.001974\n",
      "==>>> epoch: 3, batch index: 2300, train loss: 0.002406\n",
      "==>>> epoch: 3, batch index: 2400, train loss: 0.002888\n",
      "==>>> epoch: 3, batch index: 2500, train loss: 0.002164\n",
      "==>>> epoch: 3, batch index: 2600, train loss: 0.002356\n",
      "==>>> epoch: 3, batch index: 2700, train loss: 0.003470\n",
      "==>>> epoch: 3, batch index: 2800, train loss: 0.002656\n",
      "==>>> epoch: 3, batch index: 2900, train loss: 0.002733\n",
      "==>>> epoch: 3, batch index: 3000, train loss: 0.001680\n",
      "==>>> epoch: 3, batch index: 3100, train loss: 0.001917\n",
      "==>>> epoch: 3, batch index: 3200, train loss: 0.003411\n",
      "==>>> epoch: 3, batch index: 3282, train loss: 0.002456\n",
      "==>>> epoch: 3, batch index: 100, test loss: 0.000691, acc: 0.573\n",
      "==>>> epoch: 3, batch index: 200, test loss: 0.001323, acc: 0.579\n",
      "==>>> epoch: 3, batch index: 300, test loss: 0.001887, acc: 0.578\n",
      "==>>> epoch: 3, batch index: 313, test loss: 0.001956, acc: 0.578\n",
      "==>>> epoch: 4, batch index: 100, train loss: 0.002512\n",
      "==>>> epoch: 4, batch index: 200, train loss: 0.001776\n",
      "==>>> epoch: 4, batch index: 300, train loss: 0.001757\n",
      "==>>> epoch: 4, batch index: 400, train loss: 0.002414\n",
      "==>>> epoch: 4, batch index: 500, train loss: 0.003535\n",
      "==>>> epoch: 4, batch index: 600, train loss: 0.003615\n",
      "==>>> epoch: 4, batch index: 700, train loss: 0.001874\n",
      "==>>> epoch: 4, batch index: 800, train loss: 0.002397\n",
      "==>>> epoch: 4, batch index: 900, train loss: 0.002125\n",
      "==>>> epoch: 4, batch index: 1000, train loss: 0.002563\n",
      "==>>> epoch: 4, batch index: 1100, train loss: 0.001597\n",
      "==>>> epoch: 4, batch index: 1200, train loss: 0.001954\n",
      "==>>> epoch: 4, batch index: 1300, train loss: 0.001661\n",
      "==>>> epoch: 4, batch index: 1400, train loss: 0.001955\n",
      "==>>> epoch: 4, batch index: 1500, train loss: 0.001990\n",
      "==>>> epoch: 4, batch index: 1600, train loss: 0.003530\n",
      "==>>> epoch: 4, batch index: 1700, train loss: 0.002454\n",
      "==>>> epoch: 4, batch index: 1800, train loss: 0.001771\n",
      "==>>> epoch: 4, batch index: 1900, train loss: 0.002326\n",
      "==>>> epoch: 4, batch index: 2000, train loss: 0.001996\n",
      "==>>> epoch: 4, batch index: 2100, train loss: 0.002660\n",
      "==>>> epoch: 4, batch index: 2200, train loss: 0.002383\n",
      "==>>> epoch: 4, batch index: 2300, train loss: 0.002337\n",
      "==>>> epoch: 4, batch index: 2400, train loss: 0.002522\n",
      "==>>> epoch: 4, batch index: 2500, train loss: 0.002087\n",
      "==>>> epoch: 4, batch index: 2600, train loss: 0.002368\n",
      "==>>> epoch: 4, batch index: 2700, train loss: 0.002800\n",
      "==>>> epoch: 4, batch index: 2800, train loss: 0.001823\n",
      "==>>> epoch: 4, batch index: 2900, train loss: 0.002057\n",
      "==>>> epoch: 4, batch index: 3000, train loss: 0.002061\n",
      "==>>> epoch: 4, batch index: 3100, train loss: 0.001932\n",
      "==>>> epoch: 4, batch index: 3200, train loss: 0.002437\n",
      "==>>> epoch: 4, batch index: 3282, train loss: 0.003107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 4, batch index: 100, test loss: 0.000709, acc: 0.580\n",
      "==>>> epoch: 4, batch index: 200, test loss: 0.001347, acc: 0.575\n",
      "==>>> epoch: 4, batch index: 300, test loss: 0.001929, acc: 0.575\n",
      "==>>> epoch: 4, batch index: 313, test loss: 0.001988, acc: 0.575\n",
      "==>>> epoch: 5, batch index: 100, train loss: 0.003173\n",
      "==>>> epoch: 5, batch index: 200, train loss: 0.001844\n",
      "==>>> epoch: 5, batch index: 300, train loss: 0.003156\n",
      "==>>> epoch: 5, batch index: 400, train loss: 0.002374\n",
      "==>>> epoch: 5, batch index: 500, train loss: 0.002521\n",
      "==>>> epoch: 5, batch index: 600, train loss: 0.002706\n",
      "==>>> epoch: 5, batch index: 700, train loss: 0.002054\n",
      "==>>> epoch: 5, batch index: 800, train loss: 0.003122\n",
      "==>>> epoch: 5, batch index: 900, train loss: 0.002494\n",
      "==>>> epoch: 5, batch index: 1000, train loss: 0.002571\n",
      "==>>> epoch: 5, batch index: 1100, train loss: 0.002025\n",
      "==>>> epoch: 5, batch index: 1200, train loss: 0.001958\n",
      "==>>> epoch: 5, batch index: 1300, train loss: 0.002699\n",
      "==>>> epoch: 5, batch index: 1400, train loss: 0.002813\n",
      "==>>> epoch: 5, batch index: 1500, train loss: 0.002215\n",
      "==>>> epoch: 5, batch index: 1600, train loss: 0.002850\n",
      "==>>> epoch: 5, batch index: 1700, train loss: 0.002266\n",
      "==>>> epoch: 5, batch index: 1800, train loss: 0.002330\n",
      "==>>> epoch: 5, batch index: 1900, train loss: 0.002654\n",
      "==>>> epoch: 5, batch index: 2000, train loss: 0.002812\n",
      "==>>> epoch: 5, batch index: 2100, train loss: 0.002040\n",
      "==>>> epoch: 5, batch index: 2200, train loss: 0.002255\n",
      "==>>> epoch: 5, batch index: 2300, train loss: 0.002040\n",
      "==>>> epoch: 5, batch index: 2400, train loss: 0.003468\n",
      "==>>> epoch: 5, batch index: 2500, train loss: 0.001814\n",
      "==>>> epoch: 5, batch index: 2600, train loss: 0.002671\n",
      "==>>> epoch: 5, batch index: 2700, train loss: 0.001463\n",
      "==>>> epoch: 5, batch index: 2800, train loss: 0.003001\n",
      "==>>> epoch: 5, batch index: 2900, train loss: 0.002961\n",
      "==>>> epoch: 5, batch index: 3000, train loss: 0.001539\n",
      "==>>> epoch: 5, batch index: 3100, train loss: 0.002477\n",
      "==>>> epoch: 5, batch index: 3200, train loss: 0.003356\n",
      "==>>> epoch: 5, batch index: 3282, train loss: 0.001414\n",
      "==>>> epoch: 5, batch index: 100, test loss: 0.000723, acc: 0.578\n",
      "==>>> epoch: 5, batch index: 200, test loss: 0.001349, acc: 0.577\n",
      "==>>> epoch: 5, batch index: 300, test loss: 0.001904, acc: 0.575\n",
      "==>>> epoch: 5, batch index: 313, test loss: 0.001966, acc: 0.574\n",
      "==>>> epoch: 6, batch index: 100, train loss: 0.001742\n",
      "==>>> epoch: 6, batch index: 200, train loss: 0.002673\n",
      "==>>> epoch: 6, batch index: 300, train loss: 0.002411\n",
      "==>>> epoch: 6, batch index: 400, train loss: 0.002378\n",
      "==>>> epoch: 6, batch index: 500, train loss: 0.002297\n",
      "==>>> epoch: 6, batch index: 600, train loss: 0.002926\n",
      "==>>> epoch: 6, batch index: 700, train loss: 0.001853\n",
      "==>>> epoch: 6, batch index: 800, train loss: 0.001426\n",
      "==>>> epoch: 6, batch index: 900, train loss: 0.002753\n",
      "==>>> epoch: 6, batch index: 1000, train loss: 0.001454\n",
      "==>>> epoch: 6, batch index: 1100, train loss: 0.001829\n",
      "==>>> epoch: 6, batch index: 1200, train loss: 0.001560\n",
      "==>>> epoch: 6, batch index: 1300, train loss: 0.002083\n",
      "==>>> epoch: 6, batch index: 1400, train loss: 0.002034\n",
      "==>>> epoch: 6, batch index: 1500, train loss: 0.001899\n",
      "==>>> epoch: 6, batch index: 1600, train loss: 0.002352\n",
      "==>>> epoch: 6, batch index: 1700, train loss: 0.001921\n",
      "==>>> epoch: 6, batch index: 1800, train loss: 0.001751\n",
      "==>>> epoch: 6, batch index: 1900, train loss: 0.002486\n",
      "==>>> epoch: 6, batch index: 2000, train loss: 0.001829\n",
      "==>>> epoch: 6, batch index: 2100, train loss: 0.001822\n",
      "==>>> epoch: 6, batch index: 2200, train loss: 0.002242\n",
      "==>>> epoch: 6, batch index: 2300, train loss: 0.001924\n",
      "==>>> epoch: 6, batch index: 2400, train loss: 0.002992\n",
      "==>>> epoch: 6, batch index: 2500, train loss: 0.002213\n",
      "==>>> epoch: 6, batch index: 2600, train loss: 0.002190\n",
      "==>>> epoch: 6, batch index: 2700, train loss: 0.002190\n",
      "==>>> epoch: 6, batch index: 2800, train loss: 0.002745\n",
      "==>>> epoch: 6, batch index: 2900, train loss: 0.002005\n",
      "==>>> epoch: 6, batch index: 3000, train loss: 0.002888\n",
      "==>>> epoch: 6, batch index: 3100, train loss: 0.001575\n",
      "==>>> epoch: 6, batch index: 3200, train loss: 0.001851\n",
      "==>>> epoch: 6, batch index: 3282, train loss: 0.005175\n",
      "==>>> epoch: 6, batch index: 100, test loss: 0.000732, acc: 0.568\n",
      "==>>> epoch: 6, batch index: 200, test loss: 0.001381, acc: 0.566\n",
      "==>>> epoch: 6, batch index: 300, test loss: 0.001973, acc: 0.568\n",
      "==>>> epoch: 6, batch index: 313, test loss: 0.002047, acc: 0.567\n",
      "==>>> epoch: 7, batch index: 100, train loss: 0.002753\n",
      "==>>> epoch: 7, batch index: 200, train loss: 0.001698\n",
      "==>>> epoch: 7, batch index: 300, train loss: 0.002536\n",
      "==>>> epoch: 7, batch index: 400, train loss: 0.001499\n",
      "==>>> epoch: 7, batch index: 500, train loss: 0.002455\n",
      "==>>> epoch: 7, batch index: 600, train loss: 0.002324\n",
      "==>>> epoch: 7, batch index: 700, train loss: 0.002471\n",
      "==>>> epoch: 7, batch index: 800, train loss: 0.002415\n",
      "==>>> epoch: 7, batch index: 900, train loss: 0.001944\n",
      "==>>> epoch: 7, batch index: 1000, train loss: 0.001476\n",
      "==>>> epoch: 7, batch index: 1100, train loss: 0.002463\n",
      "==>>> epoch: 7, batch index: 1200, train loss: 0.002388\n",
      "==>>> epoch: 7, batch index: 1300, train loss: 0.003086\n",
      "==>>> epoch: 7, batch index: 1400, train loss: 0.002302\n",
      "==>>> epoch: 7, batch index: 1500, train loss: 0.001864\n",
      "==>>> epoch: 7, batch index: 1600, train loss: 0.002539\n",
      "==>>> epoch: 7, batch index: 1700, train loss: 0.002594\n",
      "==>>> epoch: 7, batch index: 1800, train loss: 0.002281\n",
      "==>>> epoch: 7, batch index: 1900, train loss: 0.002291\n",
      "==>>> epoch: 7, batch index: 2000, train loss: 0.002936\n",
      "==>>> epoch: 7, batch index: 2100, train loss: 0.003095\n",
      "==>>> epoch: 7, batch index: 2200, train loss: 0.002246\n",
      "==>>> epoch: 7, batch index: 2300, train loss: 0.001243\n",
      "==>>> epoch: 7, batch index: 2400, train loss: 0.002341\n",
      "==>>> epoch: 7, batch index: 2500, train loss: 0.001677\n",
      "==>>> epoch: 7, batch index: 2600, train loss: 0.002785\n",
      "==>>> epoch: 7, batch index: 2700, train loss: 0.002407\n",
      "==>>> epoch: 7, batch index: 2800, train loss: 0.003050\n",
      "==>>> epoch: 7, batch index: 2900, train loss: 0.002403\n",
      "==>>> epoch: 7, batch index: 3000, train loss: 0.002382\n",
      "==>>> epoch: 7, batch index: 3100, train loss: 0.002594\n",
      "==>>> epoch: 7, batch index: 3200, train loss: 0.001827\n",
      "==>>> epoch: 7, batch index: 3282, train loss: 0.002409\n",
      "==>>> epoch: 7, batch index: 100, test loss: 0.000687, acc: 0.573\n",
      "==>>> epoch: 7, batch index: 200, test loss: 0.001344, acc: 0.565\n",
      "==>>> epoch: 7, batch index: 300, test loss: 0.001920, acc: 0.569\n",
      "==>>> epoch: 7, batch index: 313, test loss: 0.001987, acc: 0.569\n",
      "==>>> epoch: 8, batch index: 100, train loss: 0.002386\n",
      "==>>> epoch: 8, batch index: 200, train loss: 0.002317\n",
      "==>>> epoch: 8, batch index: 300, train loss: 0.001911\n",
      "==>>> epoch: 8, batch index: 400, train loss: 0.002400\n",
      "==>>> epoch: 8, batch index: 500, train loss: 0.001901\n",
      "==>>> epoch: 8, batch index: 600, train loss: 0.001660\n",
      "==>>> epoch: 8, batch index: 700, train loss: 0.003033\n",
      "==>>> epoch: 8, batch index: 800, train loss: 0.002461\n",
      "==>>> epoch: 8, batch index: 900, train loss: 0.001740\n",
      "==>>> epoch: 8, batch index: 1000, train loss: 0.002115\n",
      "==>>> epoch: 8, batch index: 1100, train loss: 0.001710\n",
      "==>>> epoch: 8, batch index: 1200, train loss: 0.001399\n",
      "==>>> epoch: 8, batch index: 1300, train loss: 0.001902\n",
      "==>>> epoch: 8, batch index: 1400, train loss: 0.001747\n",
      "==>>> epoch: 8, batch index: 1500, train loss: 0.003222\n",
      "==>>> epoch: 8, batch index: 1600, train loss: 0.003100\n",
      "==>>> epoch: 8, batch index: 1700, train loss: 0.002053\n",
      "==>>> epoch: 8, batch index: 1800, train loss: 0.002201\n",
      "==>>> epoch: 8, batch index: 1900, train loss: 0.001916\n",
      "==>>> epoch: 8, batch index: 2000, train loss: 0.001406\n",
      "==>>> epoch: 8, batch index: 2100, train loss: 0.003487\n",
      "==>>> epoch: 8, batch index: 2200, train loss: 0.001974\n",
      "==>>> epoch: 8, batch index: 2300, train loss: 0.001935\n",
      "==>>> epoch: 8, batch index: 2400, train loss: 0.001964\n",
      "==>>> epoch: 8, batch index: 2500, train loss: 0.002012\n",
      "==>>> epoch: 8, batch index: 2600, train loss: 0.002681\n",
      "==>>> epoch: 8, batch index: 2700, train loss: 0.002102\n",
      "==>>> epoch: 8, batch index: 2800, train loss: 0.001433\n",
      "==>>> epoch: 8, batch index: 2900, train loss: 0.002130\n",
      "==>>> epoch: 8, batch index: 3000, train loss: 0.002181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 8, batch index: 3100, train loss: 0.002038\n",
      "==>>> epoch: 8, batch index: 3200, train loss: 0.002249\n",
      "==>>> epoch: 8, batch index: 3282, train loss: 0.002766\n",
      "==>>> epoch: 8, batch index: 100, test loss: 0.000696, acc: 0.570\n",
      "==>>> epoch: 8, batch index: 200, test loss: 0.001327, acc: 0.571\n",
      "==>>> epoch: 8, batch index: 300, test loss: 0.001930, acc: 0.571\n",
      "==>>> epoch: 8, batch index: 313, test loss: 0.001994, acc: 0.571\n",
      "==>>> epoch: 9, batch index: 100, train loss: 0.002130\n",
      "==>>> epoch: 9, batch index: 200, train loss: 0.002028\n",
      "==>>> epoch: 9, batch index: 300, train loss: 0.001592\n",
      "==>>> epoch: 9, batch index: 400, train loss: 0.001679\n",
      "==>>> epoch: 9, batch index: 500, train loss: 0.003028\n",
      "==>>> epoch: 9, batch index: 600, train loss: 0.001914\n",
      "==>>> epoch: 9, batch index: 700, train loss: 0.001358\n",
      "==>>> epoch: 9, batch index: 800, train loss: 0.002020\n",
      "==>>> epoch: 9, batch index: 900, train loss: 0.001768\n",
      "==>>> epoch: 9, batch index: 1000, train loss: 0.002186\n",
      "==>>> epoch: 9, batch index: 1100, train loss: 0.001769\n",
      "==>>> epoch: 9, batch index: 1200, train loss: 0.001630\n",
      "==>>> epoch: 9, batch index: 1300, train loss: 0.002429\n",
      "==>>> epoch: 9, batch index: 1400, train loss: 0.001832\n",
      "==>>> epoch: 9, batch index: 1500, train loss: 0.001907\n",
      "==>>> epoch: 9, batch index: 1600, train loss: 0.001986\n",
      "==>>> epoch: 9, batch index: 1700, train loss: 0.001861\n",
      "==>>> epoch: 9, batch index: 1800, train loss: 0.001860\n",
      "==>>> epoch: 9, batch index: 1900, train loss: 0.002034\n",
      "==>>> epoch: 9, batch index: 2000, train loss: 0.001882\n",
      "==>>> epoch: 9, batch index: 2100, train loss: 0.002383\n",
      "==>>> epoch: 9, batch index: 2200, train loss: 0.001737\n",
      "==>>> epoch: 9, batch index: 2300, train loss: 0.002171\n",
      "==>>> epoch: 9, batch index: 2400, train loss: 0.001596\n",
      "==>>> epoch: 9, batch index: 2500, train loss: 0.001863\n",
      "==>>> epoch: 9, batch index: 2600, train loss: 0.002121\n",
      "==>>> epoch: 9, batch index: 2700, train loss: 0.001945\n",
      "==>>> epoch: 9, batch index: 2800, train loss: 0.002480\n",
      "==>>> epoch: 9, batch index: 2900, train loss: 0.001764\n",
      "==>>> epoch: 9, batch index: 3000, train loss: 0.001612\n",
      "==>>> epoch: 9, batch index: 3100, train loss: 0.001635\n",
      "==>>> epoch: 9, batch index: 3200, train loss: 0.001486\n",
      "==>>> epoch: 9, batch index: 3282, train loss: 0.003631\n",
      "==>>> epoch: 9, batch index: 100, test loss: 0.000699, acc: 0.579\n",
      "==>>> epoch: 9, batch index: 200, test loss: 0.001360, acc: 0.573\n",
      "==>>> epoch: 9, batch index: 300, test loss: 0.001910, acc: 0.573\n",
      "==>>> epoch: 9, batch index: 313, test loss: 0.001974, acc: 0.572\n",
      "==>>> epoch: 10, batch index: 100, train loss: 0.002244\n",
      "==>>> epoch: 10, batch index: 200, train loss: 0.002477\n",
      "==>>> epoch: 10, batch index: 300, train loss: 0.002481\n",
      "==>>> epoch: 10, batch index: 400, train loss: 0.001763\n",
      "==>>> epoch: 10, batch index: 500, train loss: 0.001906\n",
      "==>>> epoch: 10, batch index: 600, train loss: 0.001939\n",
      "==>>> epoch: 10, batch index: 700, train loss: 0.001786\n",
      "==>>> epoch: 10, batch index: 800, train loss: 0.002489\n",
      "==>>> epoch: 10, batch index: 900, train loss: 0.001789\n",
      "==>>> epoch: 10, batch index: 1000, train loss: 0.002054\n",
      "==>>> epoch: 10, batch index: 1100, train loss: 0.003045\n",
      "==>>> epoch: 10, batch index: 1200, train loss: 0.002160\n",
      "==>>> epoch: 10, batch index: 1300, train loss: 0.003883\n",
      "==>>> epoch: 10, batch index: 1400, train loss: 0.002027\n",
      "==>>> epoch: 10, batch index: 1500, train loss: 0.001981\n",
      "==>>> epoch: 10, batch index: 1600, train loss: 0.002002\n",
      "==>>> epoch: 10, batch index: 1700, train loss: 0.002539\n",
      "==>>> epoch: 10, batch index: 1800, train loss: 0.002440\n",
      "==>>> epoch: 10, batch index: 1900, train loss: 0.002148\n",
      "==>>> epoch: 10, batch index: 2000, train loss: 0.001805\n",
      "==>>> epoch: 10, batch index: 2100, train loss: 0.001936\n",
      "==>>> epoch: 10, batch index: 2200, train loss: 0.002537\n",
      "==>>> epoch: 10, batch index: 2300, train loss: 0.001961\n",
      "==>>> epoch: 10, batch index: 2400, train loss: 0.001788\n",
      "==>>> epoch: 10, batch index: 2500, train loss: 0.001748\n",
      "==>>> epoch: 10, batch index: 2600, train loss: 0.002160\n",
      "==>>> epoch: 10, batch index: 2700, train loss: 0.001984\n",
      "==>>> epoch: 10, batch index: 2800, train loss: 0.002414\n",
      "==>>> epoch: 10, batch index: 2900, train loss: 0.001853\n",
      "==>>> epoch: 10, batch index: 3000, train loss: 0.001499\n",
      "==>>> epoch: 10, batch index: 3100, train loss: 0.002217\n",
      "==>>> epoch: 10, batch index: 3200, train loss: 0.002242\n",
      "==>>> epoch: 10, batch index: 3282, train loss: 0.003041\n",
      "==>>> epoch: 10, batch index: 100, test loss: 0.000730, acc: 0.564\n",
      "==>>> epoch: 10, batch index: 200, test loss: 0.001371, acc: 0.568\n",
      "==>>> epoch: 10, batch index: 300, test loss: 0.001953, acc: 0.565\n",
      "==>>> epoch: 10, batch index: 313, test loss: 0.002018, acc: 0.565\n",
      "==>>> epoch: 11, batch index: 100, train loss: 0.002012\n",
      "==>>> epoch: 11, batch index: 200, train loss: 0.002105\n",
      "==>>> epoch: 11, batch index: 300, train loss: 0.001848\n",
      "==>>> epoch: 11, batch index: 400, train loss: 0.001918\n",
      "==>>> epoch: 11, batch index: 500, train loss: 0.001795\n",
      "==>>> epoch: 11, batch index: 600, train loss: 0.001684\n",
      "==>>> epoch: 11, batch index: 700, train loss: 0.002045\n",
      "==>>> epoch: 11, batch index: 800, train loss: 0.002519\n",
      "==>>> epoch: 11, batch index: 900, train loss: 0.001775\n",
      "==>>> epoch: 11, batch index: 1000, train loss: 0.001562\n",
      "==>>> epoch: 11, batch index: 1100, train loss: 0.002175\n",
      "==>>> epoch: 11, batch index: 1200, train loss: 0.002234\n",
      "==>>> epoch: 11, batch index: 1300, train loss: 0.002710\n",
      "==>>> epoch: 11, batch index: 1400, train loss: 0.002276\n",
      "==>>> epoch: 11, batch index: 1500, train loss: 0.002622\n",
      "==>>> epoch: 11, batch index: 1600, train loss: 0.001893\n",
      "==>>> epoch: 11, batch index: 1700, train loss: 0.001930\n",
      "==>>> epoch: 11, batch index: 1800, train loss: 0.002489\n",
      "==>>> epoch: 11, batch index: 1900, train loss: 0.003649\n",
      "==>>> epoch: 11, batch index: 2000, train loss: 0.002037\n",
      "==>>> epoch: 11, batch index: 2100, train loss: 0.002664\n",
      "==>>> epoch: 11, batch index: 2200, train loss: 0.002033\n",
      "==>>> epoch: 11, batch index: 2300, train loss: 0.001548\n",
      "==>>> epoch: 11, batch index: 2400, train loss: 0.002202\n",
      "==>>> epoch: 11, batch index: 2500, train loss: 0.001642\n",
      "==>>> epoch: 11, batch index: 2600, train loss: 0.001874\n",
      "==>>> epoch: 11, batch index: 2700, train loss: 0.002269\n",
      "==>>> epoch: 11, batch index: 2800, train loss: 0.002217\n",
      "==>>> epoch: 11, batch index: 2900, train loss: 0.001593\n",
      "==>>> epoch: 11, batch index: 3000, train loss: 0.003247\n",
      "==>>> epoch: 11, batch index: 3100, train loss: 0.001715\n",
      "==>>> epoch: 11, batch index: 3200, train loss: 0.001749\n",
      "==>>> epoch: 11, batch index: 3282, train loss: 0.001492\n",
      "==>>> epoch: 11, batch index: 100, test loss: 0.000715, acc: 0.566\n",
      "==>>> epoch: 11, batch index: 200, test loss: 0.001381, acc: 0.564\n",
      "==>>> epoch: 11, batch index: 300, test loss: 0.001963, acc: 0.562\n",
      "==>>> epoch: 11, batch index: 313, test loss: 0.002042, acc: 0.562\n",
      "==>>> epoch: 12, batch index: 100, train loss: 0.001741\n",
      "==>>> epoch: 12, batch index: 200, train loss: 0.001482\n",
      "==>>> epoch: 12, batch index: 300, train loss: 0.002005\n",
      "==>>> epoch: 12, batch index: 400, train loss: 0.001941\n",
      "==>>> epoch: 12, batch index: 500, train loss: 0.001298\n",
      "==>>> epoch: 12, batch index: 600, train loss: 0.002108\n",
      "==>>> epoch: 12, batch index: 700, train loss: 0.002009\n",
      "==>>> epoch: 12, batch index: 800, train loss: 0.001466\n",
      "==>>> epoch: 12, batch index: 900, train loss: 0.001804\n",
      "==>>> epoch: 12, batch index: 1000, train loss: 0.001707\n"
     ]
    }
   ],
   "source": [
    "global epoch #declear epoch global, to be used later by torch.save() \n",
    "f1_eval_storage = {'prediction':[], 'target':[]}\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    ave_loss = 0\n",
    "    global loss #declear loss global, to be used later by torch.save() \n",
    "    \n",
    "\n",
    "    for batch_idx, diction in enumerate(train_loader):\n",
    "        \n",
    "        model.train() #set model to traning mode\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, target = diction['image'], diction['rating'] #extract training data for this batch\n",
    "        x, target = x.float(), target.float() #set datatype\n",
    "        x, target = x.to(device), target.to(device) #transfer to GPU\n",
    "        x, target = Variable(x), Variable(target) #set to pytorch datatype: variable\n",
    "        \n",
    "        with autocast():\n",
    "            out = model(x) #forward pass\n",
    "            loss = criterion(out.squeeze(1), target / 10) #calculate loss\n",
    "            \n",
    "        ave_loss = ave_loss * 0.999 + loss.item() * 0.001 \n",
    "        \n",
    "        scaler.scale(loss).backward() #back propagation with calculated loss\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(epoch, batch_idx + 1, loss))\n",
    "        if (batch_idx + 1) == len(train_loader):\n",
    "            training_stats['tarining_loss'].append(ave_loss)\n",
    "\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    \n",
    "    for batch_idx, diction in enumerate(val_loader):\n",
    "        model.eval() #set model to evaluation mode\n",
    "        \n",
    "        x, target = diction['image'], diction['rating']\n",
    "        x, target = x.float(), target.float()\n",
    "        x, target = x.to(device), target.to(device) #transfer to GPU\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out, target / 10)\n",
    "        pred_label = np.around(out.cpu().detach().numpy() * 10)\n",
    "        \n",
    "        f1_eval_storage['prediction'] = np.append(f1_eval_storage['prediction'], pred_label.squeeze(1))\n",
    "        f1_eval_storage['target'] = np.append(f1_eval_storage['target'], target.cpu().detach().numpy())\n",
    "        \n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label.squeeze(1) == target.cpu().detach().numpy()).sum()\n",
    "        ave_loss = ave_loss * 0.999 + loss.item() * 0.001 #smooth average\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(val_loader):\n",
    "            print(\n",
    "            '==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx + 1, ave_loss, correct_cnt.item() * 1.0 / total_cnt))\n",
    "        if (batch_idx + 1) == len(val_loader):\n",
    "            training_stats['validation_loss'].append(ave_loss)\n",
    "            training_stats['accuracy'].append(correct_cnt.item() * 1.0 / total_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       0.20      0.03      0.05        37\n",
      "         2.0       0.00      0.00      0.00         6\n",
      "         3.0       0.04      0.04      0.04        48\n",
      "         4.0       0.34      0.17      0.22      1172\n",
      "         5.0       0.64      0.74      0.68     10852\n",
      "         6.0       0.56      0.50      0.53      7259\n",
      "         7.0       0.18      0.05      0.08       578\n",
      "         8.0       0.00      0.00      0.00        41\n",
      "         9.0       0.00      0.00      0.00         4\n",
      "        10.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.60     19999\n",
      "   macro avg       0.18      0.14      0.15     19999\n",
      "weighted avg       0.57      0.60      0.58     19999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(f1_eval_storage['target'], f1_eval_storage['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pthflops import count_ops\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inp = torch.Tensor(np.zeros(150528).reshape(1, 3, 224, 224))\n",
    "\n",
    "inp = inp.cuda()\n",
    "inp = inp.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "count_ops(model, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Densenet169_Native_Benchmark\"\n",
    "checkpoint_save_dir = r\"C:\\Users\\Leo's PC\\PycharmProjects\\PD\\Model Checkpoints\"\n",
    "checkpoint_file = open(checkpoint_save_dir + \"\\\\\" + model_name + \"_\" + \"E\" + str(epoch) + \"_\" + time.strftime(\"%m.%d.%Y_%H.%M.%S\") \n",
    "                       + \".tar\", 'wb')\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, checkpoint_file)\n",
    "\n",
    "checkpoint_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.ylabel('Training loss/Validation loss/Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training loss, Validation loss, Accuracy vs Epoch')\n",
    "\n",
    "plt.plot(np.arange(len(training_stats['tarining_loss'])), training_stats['tarining_loss'])\n",
    "plt.plot(np.arange(len(training_stats['validation_loss'])), training_stats['validation_loss'])\n",
    "#plt.plot(np.arange(len(training_stats['accuracy'])), training_stats['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(open(checkpoint_save_dir + \"\\LN_VGG16_E0_06.27.2020_16.25.57.tar\", 'rb'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = AVA[25000]\n",
    "print(i, sample['image'].size(), sample['rating'])\n",
    "img = sample['image'][1]\n",
    "img = np.array(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after input. size: torch.Size([2, 64, 56, 56])\n",
    "after block1. size: torch.Size([2, 128, 28, 28])\n",
    "after block2. size: torch.Size([2, 256, 14, 14])\n",
    "after block3. size: torch.Size([2, 640, 7, 7])\n",
    "after block4. size: torch.Size([2, 1664, 7, 7])\n",
    "after pooling. size: torch.Size([2, 1664, 1, 1])\n",
    "\n",
    ".resize_([batch_size, 1])\n",
    ".resize_([batch_size, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
